{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About this notebook\n- [Luke](https://arxiv.org/pdf/2010.01057v1.pdf)-base starter notebook\n- [Training notebook](https://www.kaggle.com/yasufuminakama/jigsaw4-luke-base-starter-train)\n- Approach References\n    - https://www.kaggle.com/c/jigsaw-toxic-severity-rating/discussion/286471\n    - https://www.kaggle.com/debarshichanda/pytorch-w-b-jigsaw-starter\n    - https://www.kaggle.com/debarshichanda/0-816-jigsaw-inference\n    - Thanks for sharing @debarshichanda","metadata":{}},{"cell_type":"markdown","source":"# Directory settings","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Directory settings\n# ====================================================\nimport os\n\nOUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:58:29.573375Z","iopub.execute_input":"2022-02-01T07:58:29.573688Z","iopub.status.idle":"2022-02-01T07:58:29.597682Z","shell.execute_reply.started":"2022-02-01T07:58:29.57362Z","shell.execute_reply":"2022-02-01T07:58:29.597102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CFG","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    model_dir='../input/luke-0860/jigsaw-server-ruddit-luke/'\n    num_workers=4\n    model=\"studio-ousia/luke-base\"\n    batch_size=128\n    fc_dropout=0.\n    text=\"text\"\n    target=\"target\"\n    target_size=1\n    head=32\n    tail=32\n    seed=42\n    n_fold=5\n\n\nCFG.max_len = CFG.head + CFG.tail","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:58:32.194549Z","iopub.execute_input":"2022-02-01T07:58:32.195075Z","iopub.status.idle":"2022-02-01T07:58:32.200021Z","shell.execute_reply.started":"2022-02-01T07:58:32.195037Z","shell.execute_reply":"2022-02-01T07:58:32.199147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport re\nimport sys\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nos.system('pip uninstall -q transformers -y')\nos.system('pip uninstall -q tokenizers -y')\nos.system('pip uninstall -q huggingface_hub -y')\n\nos.system('mkdir -p /tmp/pip/cache-tokenizers/')\nos.system('cp ../input/tokenizers-0103/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl /tmp/pip/cache-tokenizers/')\nos.system('pip install -q --no-index --find-links /tmp/pip/cache-tokenizers/ tokenizers')\n\nos.system('mkdir -p /tmp/pip/cache-huggingface-hub/')\nos.system('cp ../input/huggingface-hub-008/huggingface_hub-0.0.8-py3-none-any.whl /tmp/pip/cache-huggingface-hub/')\nos.system('pip install -q --no-index --find-links /tmp/pip/cache-huggingface-hub/ huggingface_hub')\n\nos.system('mkdir -p /tmp/pip/cache-transformers/')\nos.system('cp ../input/transformers-470/transformers-4.7.0-py3-none-any.whl /tmp/pip/cache-transformers/')\nos.system('pip install -q --no-index --find-links /tmp/pip/cache-transformers/ transformers')\n\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import LukeTokenizer, LukeModel, LukeConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:58:32.517398Z","iopub.execute_input":"2022-02-01T07:58:32.517629Z","iopub.status.idle":"2022-02-01T07:59:04.836551Z","shell.execute_reply.started":"2022-02-01T07:58:32.517603Z","shell.execute_reply":"2022-02-01T07:59:04.835761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Utils\n# ====================================================\ndef get_score(df):\n    score = len(df[df['less_toxic_pred'] < df['more_toxic_pred']]) / len(df)\n    return score\n\n\ndef get_logger(filename=OUTPUT_DIR+'train'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:59:04.838692Z","iopub.execute_input":"2022-02-01T07:59:04.83909Z","iopub.status.idle":"2022-02-01T07:59:04.850894Z","shell.execute_reply.started":"2022-02-01T07:59:04.839053Z","shell.execute_reply":"2022-02-01T07:59:04.850207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Data Loading\n# ====================================================\ntest = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\nsubmission = pd.read_csv('../input/jigsaw-toxic-severity-rating/sample_submission.csv')\nprint(test.shape, submission.shape)\ndisplay(test.head())\ndisplay(submission.head())","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:59:04.853816Z","iopub.execute_input":"2022-02-01T07:59:04.854279Z","iopub.status.idle":"2022-02-01T07:59:05.004123Z","shell.execute_reply.started":"2022-02-01T07:59:04.854251Z","shell.execute_reply":"2022-02-01T07:59:05.003328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# tokenizer","metadata":{}},{"cell_type":"code","source":"CFG.tokenizer = LukeTokenizer.from_pretrained(CFG.model_dir+'tokenizer/')","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:59:05.006228Z","iopub.execute_input":"2022-02-01T07:59:05.006607Z","iopub.status.idle":"2022-02-01T07:59:06.100154Z","shell.execute_reply.started":"2022-02-01T07:59:05.006566Z","shell.execute_reply":"2022-02-01T07:59:06.099416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input(text, cfg):\n    if cfg.tail == 0:\n        inputs = cfg.tokenizer.encode_plus(text, \n                                           return_tensors=None, \n                                           add_special_tokens=True, \n                                           max_length=cfg.max_len,\n                                           pad_to_max_length=True,\n                                           truncation=True)\n        for k, v in inputs.items():\n            inputs[k] = torch.tensor(v, dtype=torch.long)\n    else:\n        inputs = cfg.tokenizer.encode_plus(text,\n                                           return_tensors=None, \n                                           add_special_tokens=True, \n                                           truncation=True)\n        for k, v in inputs.items():\n            v_length = len(v)\n            if v_length > cfg.max_len:\n                v = np.hstack([v[:cfg.head], v[-cfg.tail:]])\n            if k == 'input_ids':\n                new_v = np.ones(cfg.max_len) * cfg.tokenizer.pad_token_id\n            else:\n                new_v = np.zeros(cfg.max_len)\n            new_v[:v_length] = v \n            inputs[k] = torch.tensor(new_v, dtype=torch.long)\n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.text = df[cfg.text].fillna(\"none\").values\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        text = str(self.text[item])\n        inputs = prepare_input(text, self.cfg)\n        return inputs","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:59:06.101375Z","iopub.execute_input":"2022-02-01T07:59:06.103558Z","iopub.status.idle":"2022-02-01T07:59:06.113628Z","shell.execute_reply.started":"2022-02-01T07:59:06.103526Z","shell.execute_reply":"2022-02-01T07:59:06.112877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = LukeConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = LukeModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = LukeModel(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, cfg.target_size)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        feature = torch.mean(last_hidden_states, 1)\n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:59:06.114936Z","iopub.execute_input":"2022-02-01T07:59:06.115625Z","iopub.status.idle":"2022-02-01T07:59:06.129228Z","shell.execute_reply.started":"2022-02-01T07:59:06.115587Z","shell.execute_reply":"2022-02-01T07:59:06.128504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# inference","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# inference\n# ====================================================\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:59:06.130609Z","iopub.execute_input":"2022-02-01T07:59:06.130999Z","iopub.status.idle":"2022-02-01T07:59:06.141163Z","shell.execute_reply.started":"2022-02-01T07:59:06.130921Z","shell.execute_reply":"2022-02-01T07:59:06.140309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, \n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\nconfig_path = CFG.model_dir+\"config.pth\"\npredictions = []\nfor fold in range(CFG.n_fold):\n    model = CustomModel(CFG, config_path=config_path, pretrained=False)\n    state = torch.load(CFG.model_dir+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state; gc.collect()\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T08:23:54.778635Z","iopub.execute_input":"2022-02-01T08:23:54.779258Z","iopub.status.idle":"2022-02-01T08:26:23.750849Z","shell.execute_reply.started":"2022-02-01T08:23:54.779218Z","shell.execute_reply":"2022-02-01T08:26:23.750103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions1=np.mean(predictions, axis=0)\npredictions1=np.squeeze(predictions1,axis=1)\ndel predictions","metadata":{"execution":{"iopub.status.busy":"2022-02-01T08:26:23.755292Z","iopub.execute_input":"2022-02-01T08:26:23.755503Z","iopub.status.idle":"2022-02-01T08:26:23.796278Z","shell.execute_reply.started":"2022-02-01T08:26:23.755479Z","shell.execute_reply":"2022-02-01T08:26:23.795509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# tfidf","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict\nimport time\nimport scipy.optimize as optimize\nimport lightgbm as lgb\nfrom bs4 import BeautifulSoup\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import rankdata\n\nfrom collections import defaultdict\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom tqdm import tqdm\n\nimport time\nimport re \nimport scipy\nfrom scipy import sparse\nimport gc \nfrom IPython.display import display, HTML\nfrom pprint import pprint\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-02-01T08:03:34.050794Z","iopub.execute_input":"2022-02-01T08:03:34.05106Z","iopub.status.idle":"2022-02-01T08:03:36.20145Z","shell.execute_reply.started":"2022-02-01T08:03:34.051019Z","shell.execute_reply":"2022-02-01T08:03:36.200699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Fold_type=2\nis_test=False\ntranslate_aug=False\nfast_vec=True\n#记得改kdict 里面的tranlate的值\n\nfold_num_k=5\nfold_num_s=1\n\nfast_factor=1","metadata":{"execution":{"iopub.status.busy":"2022-02-01T08:03:36.202818Z","iopub.execute_input":"2022-02-01T08:03:36.203093Z","iopub.status.idle":"2022-02-01T08:03:36.208771Z","shell.execute_reply.started":"2022-02-01T08:03:36.203047Z","shell.execute_reply":"2022-02-01T08:03:36.208078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# data_names=[\"jc_\",\"rud_\",\"jcc1_\",\"jcc2_\",\"jc_fr_\",\"jc_de_\",\"jc_es_\"]\n# data_names=[\"jc_\",\"rud_\"]\ndata_names=[\"jc_\",\"rud_\",\"jcc1_\",\"jcc2_\"]\n\njc_num,jc_rate,rud_rate=3,4,1\nfactor_data=[jc_rate/jc_num,rud_rate,jc_rate/jc_num,jc_rate/jc_num,jc_rate/jc_num,jc_rate/jc_num,jc_rate/jc_num]\n\n#这里是用于翻译增强 可以不用管\ntranslate_data=[\"jc_s_\"]\ntranslate_language=[\"fr_text\",\"de_text\",\"es_text\"]\nclean_data={\"jcc1_s_\":1,\"jcc2_s_\":2}\n\n#这里是选择模型 可选参数 包括 ridge 和gbm factor用于指定相应权值 ，gbm重复试验的结果看效果不好，选择放弃\nmodel_choice=[\"ridge\"]\nfactor=[1]\n\n\nsystem_path=r\"../input\"\nmodel_all=[\"ridge\",\"gbm\"]\nfor model_name in model_all:\n    if model_name not in os.listdir():\n        os.makedirs(f\"{model_name}\")\nout_path=r\"./\"","metadata":{"execution":{"iopub.status.busy":"2022-02-01T08:03:36.210268Z","iopub.execute_input":"2022-02-01T08:03:36.210745Z","iopub.status.idle":"2022-02-01T08:03:36.219916Z","shell.execute_reply.started":"2022-02-01T08:03:36.210707Z","shell.execute_reply":"2022-02-01T08:03:36.219231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATA","metadata":{}},{"cell_type":"code","source":"#第一届 jigsaw比赛 数据（challenge） Toxic Comment Classification Challenge\n\njc_path=os.path.join(system_path,\"jigsaw-toxic-comment-classification-challenge\")\njc_trans_path=os.path.join(system_path,\"jc-trans\")\n#ruddit 数据\\\nrun_path=os.path.join(system_path,\"ruddit-jigsaw-dataset/Dataset\")\n#第二届 jigsaw比赛 对少数人群不歧视\njuc_path=os.path.join(system_path,\"jigsaw-unintended-bias-in-toxicity-classification\")\n\n#本次比赛数据 作为val\njts_path=os.path.join(system_path,\"jigsaw-toxic-severity-rating\")\n\n# #数据抽样存储路径\ngbm_save_path=os.path.join(out_path,\"gbm\")\nridge_save_path=os.path.join(out_path,\"ridge\")","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:18:19.846680Z","iopub.execute_input":"2022-02-01T10:18:19.847249Z","iopub.status.idle":"2022-02-01T10:18:19.930531Z","shell.execute_reply.started":"2022-02-01T10:18:19.847160Z","shell.execute_reply":"2022-02-01T10:18:19.929201Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#验证集和测试集\ndf_val = pd.read_csv(os.path.join(jts_path,\"validation_data.csv\"))\n\ndf_test = pd.read_csv(os.path.join(jts_path,\"comments_to_score.csv\"))","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:18:19.931409Z","iopub.status.idle":"2022-02-01T10:18:19.931854Z","shell.execute_reply.started":"2022-02-01T10:18:19.931636Z","shell.execute_reply":"2022-02-01T10:18:19.931660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#第一届比赛数据 以0/1为分值 \nfeatures = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n\njc_train_df = pd.read_csv(os.path.join(jc_path,\"train.csv\"))\njc_test_df = pd.read_csv(os.path.join(jc_path,\"test.csv\"))\ntemp_df = pd.read_csv(os.path.join(jc_path,\"test_labels.csv\"))\n\njc_test_df = jc_test_df.merge ( temp_df, on =\"id\")\n#drop test data not used for scoring\njc_test_df = jc_test_df.query (\"toxic != -1\")\njc_test_df=jc_test_df[jc_test_df[features].sum(axis=1)>0]\n\njc_df = jc_train_df.append ( jc_test_df ) \n\n# jc_df=jc_train_df\n\n# 将代表有毒行为的筛选出来\njc_df[\"toxic_subtype_sum\"]=jc_df[features].sum(axis=1)\njc_df[\"toxic_behaviour\"]=jc_df[\"toxic_subtype_sum\"].map(lambda x: x > 0)\n\ntot_toxic_behaviour = jc_df[\"toxic_behaviour\"].sum()\nprint(f'comments with toxic behaviour:{tot_toxic_behaviour}')\njc_df=jc_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:18:19.933073Z","iopub.status.idle":"2022-02-01T10:18:19.933605Z","shell.execute_reply.started":"2022-02-01T10:18:19.933358Z","shell.execute_reply":"2022-02-01T10:18:19.933407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#第一届比赛 数据预处理\n# toxic = 1.0\n# severe_toxic = 2.0\n# obscene = 1.0\n# threat = 1.0\n# insult = 1.0\n# identity_hate = 2.0\ncat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n            'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\nPSEUDO_LABEL_WEIGHT = 0.033\ntoxic_labels=[k for k in cat_mtpl.keys()]\ndef create_train (df):\n    \n    for category in cat_mtpl:\n        df[category] = df[category] * cat_mtpl[category]\n    df['y'] = df.loc[:, toxic_labels].sum(axis=1)\n    \n    \n    df = df[[\"id\",'comment_text', 'y', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].rename(columns={'comment_text': 'text'})\n    return df\n        \njc_df = create_train (jc_df)\njc_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:18:19.935346Z","iopub.status.idle":"2022-02-01T10:18:19.936277Z","shell.execute_reply.started":"2022-02-01T10:18:19.936042Z","shell.execute_reply":"2022-02-01T10:18:19.936068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ruddit 数据\nrud_df = pd.read_csv(os.path.join(run_path,\"ruddit_with_text.csv\"))\n\nprint(f\"rud_df:{rud_df.shape}\")\nrud_df['y'] = rud_df['offensiveness_score'].map(lambda x: 0.0 if x <=0 else x)\n# rud_df['y'] = rud_df['offensiveness_score']\n\nrud_df = rud_df[['txt', 'y']].rename(columns={'txt': 'text'})\ndele_flag=\"[deleted]\"\nrud_df=rud_df.query(\"text!=@dele_flag\")","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:18:19.937498Z","iopub.status.idle":"2022-02-01T10:18:19.938458Z","shell.execute_reply.started":"2022-02-01T10:18:19.938120Z","shell.execute_reply":"2022-02-01T10:18:19.938145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#clean data\ndef clean1(text):\n\n    # Clean some punctutations\n    text=re.sub(r'\\n', r' \\n ',text)\n    text=re.sub(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3',text)\n    \n    # Replace repeating characters more than 3 times to length of 3\n    text=re.sub(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1',text)\n    \n    # Add space around repeating characters\n    text=re.sub(r'([*!?\\']+)',r' \\1 ',text)\n    \n    # patterns with repeating characters \n    text=re.sub(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1',text)\n    text=re.sub(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1',text)\n    text=re.sub(r'[ ]{2,}',' ',text)\n    text = text.strip()\n    \n    return text\n\n\ndef clean2(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\ndef clean3(text):\n    '''\n    1+2\n    '''\n    text=re.sub(r'\\n', r' \\n ',text)\n    text=re.sub(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3',text)\n    \n    # Replace repeating characters more than 3 times to length of 3\n    text=re.sub(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1',text)\n    \n    # Add space around repeating characters\n    text=re.sub(r'([*!?\\']+)',r' \\1 ',text)\n    \n    # patterns with repeating characters \n    text=re.sub(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1',text)\n    text=re.sub(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1',text)\n    text=re.sub(r'[ ]{2,}',' ',text)\n    text = text.strip()\n    \n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n\ntqdm.pandas()\n# clean 数据\ndef clean_df(df,clean_type):\n    clean_df=df.copy()\n    \n    if clean_type==1:\n        \n        clean_df['text']=clean_df['text'].progress_apply(clean1)\n    elif clean_type==2:\n        \n        clean_df['text']=clean_df['text'].progress_apply(clean2)\n    elif clean_type==3:\n        clean_df['text']=clean_df['text'].progress_apply(clean3)\n    return clean_df\njcc1_df=clean_df(jc_df,1)\njcc2_df=clean_df(jc_df,2)\n# jcc3_df=clean_df(jc_df,3)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:18:19.939982Z","iopub.status.idle":"2022-02-01T10:18:19.940578Z","shell.execute_reply.started":"2022-02-01T10:18:19.940247Z","shell.execute_reply":"2022-02-01T10:18:19.940271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_fold_sample(df,n_folds=3,frac_1=0.8,frac_1_factor=1.5,select_num=0,balance=False,translate=False):\n    df_dict={}\n    #正样本 大于等于select_num\n    select_list=-(df['y'] <select_num)\n    min_len=select_list.sum()\n    if translate==True:\n        min_len=4*min_len\n    for fld in range(n_folds):\n        if balance==False:\n#             df_y0_undersample=df[df.y<select_num].sample(n=int(min_len*frac_1*frac_1_factor), random_state = 10*(fld+1))\n            df_y0_undersample=df[df.y<select_num].sample(n=int(min_len*frac_1*frac_1_factor), random_state = 201)\n\n            tmp_df = pd.concat([df[select_list].sample(frac=frac_1, random_state = 10*(fld+1)),df_y0_undersample ])\n        else:\n            tmp_df = df.sample(frac=frac_1, random_state = 10*(fld+1))\n            \n        df_dict[fld]=tmp_df\n\n    return df_dict\n\nif is_test==True:\n    jc_df=jc_df[0:400]\n    rud_df=rud_df[0:400]\n    jcc_df=jcc_df[0:400]","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:18:19.965179Z","iopub.execute_input":"2022-02-01T10:18:19.965456Z","iopub.status.idle":"2022-02-01T10:18:19.981859Z","shell.execute_reply.started":"2022-02-01T10:18:19.965431Z","shell.execute_reply":"2022-02-01T10:18:19.980877Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"if is_test==True:\n\n    jc_df_sdict=create_fold_sample(jc_df,n_folds=fold_num_s,frac_1=0.8,frac_1_factor=1.5,select_num=0.001,balance=True,translate=translate_aug)\n    rud_df_sdict=create_fold_sample(rud_df,n_folds=fold_num_s,frac_1=0.8,frac_1_factor=1.5,select_num=0.5,balance=True)\n    jcc1_df_sdict=create_fold_sample(jcc1_df,n_folds=fold_num_s,frac_1=0.8,frac_1_factor=1.5,select_num=0.001,balance=True)\n    jcc2_df_sdict=create_fold_sample(jcc2_df,n_folds=fold_num_s,frac_1=0.8,frac_1_factor=1.5,select_num=0.001,balance=True)\n    \nif is_test==False:\n\n    jc_df_sdict=create_fold_sample(jc_df,n_folds=fold_num_s,frac_1=1,frac_1_factor=1.5,select_num=0.001,balance=False,translate=translate_aug)\n    rud_df_sdict=create_fold_sample(rud_df,n_folds=fold_num_s,frac_1=1,frac_1_factor=1,select_num=0.5,balance=True)\n    jcc1_df_sdict=create_fold_sample(jcc1_df,n_folds=fold_num_s,frac_1=1,frac_1_factor=1.5,select_num=0.001,balance=False)\n    jcc2_df_sdict=create_fold_sample(jcc2_df,n_folds=fold_num_s,frac_1=1,frac_1_factor=1.5,select_num=0.001,balance=False)\n\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:18:19.985423Z","iopub.execute_input":"2022-02-01T10:18:19.986137Z","iopub.status.idle":"2022-02-01T10:18:20.004094Z","shell.execute_reply.started":"2022-02-01T10:18:19.986103Z","shell.execute_reply":"2022-02-01T10:18:20.002786Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from scipy.sparse import hstack\nfrom scipy import sparse\nfrom gensim.models import KeyedVectors, FastText\n\nfmodel = FastText.load('../input/jigsaw-regression-based-data/FastText-jigsaw-256D/Jigsaw-Fasttext-Word-Embeddings-256D.bin')\ndef splitter(text):\n    tokens = []\n    \n    for word in text.split(' '):\n        tokens.append(word)\n    \n    return tokens\n\ndef vectorizer(text):\n    tokens = splitter(text)\n    \n    x = np.mean(fmodel.wv[tokens], axis = 0).reshape(1, -1)\n    \n    return np.squeeze(x,axis=0)   \ndef text2fasttextarray(text):\n    X_fast=[]\n    for t in text:\n        X_fast.append(vectorizer(t))\n    X_fast=np.matrix(X_fast)\n    \n    return X_fast","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:18:20.005049Z","iopub.status.idle":"2022-02-01T10:18:20.005626Z","shell.execute_reply.started":"2022-02-01T10:18:20.005434Z","shell.execute_reply":"2022-02-01T10:18:20.005458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RIDGE","metadata":{}},{"cell_type":"code","source":"import joblib\ndef ridge_cv(df_dic,n_folds,model_pre=\"jc_k_ridge_\",df_val=df_val,clean_prm=0,translate=False):\n    val_preds_arr1 = np.zeros((df_val.shape[0], n_folds))\n    val_preds_arr2 = np.zeros((df_val.shape[0], n_folds))\n    test_preds_arr = np.zeros((df_test.shape[0], n_folds))\n    for fld in tqdm(range(n_folds)):\n        df = df_dic[fld]\n        vec = TfidfVectorizer(analyzer='char_wb', max_df=0.5, min_df=3, ngram_range=(3, 5) )\n        vec_pre=model_pre+\"vec_\"\n        if translate!=True:\n            text=df[\"text\"]\n            y=df[\"y\"]\n        else:\n            trans_df=df.dropna(axis=0,subset = [\"fr_text\"])\n            text=pd.concat([df[\"text\"],trans_df[\"fr_text\"],trans_df[\"es_text\"],trans_df[\"de_text\"]])\n            y=pd.concat([df[\"y\"],trans_df[\"y\"],trans_df[\"y\"],trans_df[\"y\"]])\n        X=vec.fit_transform(text)\n#         joblib.dump(vec,os.path.join(ridge_save_path,f'{vec_pre}{fld}.pkl')) #保存模型 文件后缀为.pkl\n        model=Ridge(alpha=0.5)\n        model.fit(X,y)\n        if fast_vec==True:\n            X_f=text2fasttextarray(text)\n            model_f=Ridge(alpha=0.5)\n            model_f.fit(X_f,y)\n        if clean_prm==1:\n            X_less_toxic = df_val.apply(lambda row :clean1(row[\"less_toxic\"]),axis=1)\n            X_more_toxic = df_val.apply(lambda row :clean1(row[\"more_toxic\"]),axis=1)\n            X_test = df_test.apply(lambda row :clean1(row[\"text\"]),axis=1)\n    \n        elif clean_prm==2:\n            X_less_toxic = df_val.apply(lambda row :clean2(row[\"less_toxic\"]),axis=1)\n            X_more_toxic = df_val.apply(lambda row :clean2(row[\"more_toxic\"]),axis=1)\n            X_test = df_test.apply(lambda row :clean2(row[\"text\"]),axis=1)\n        elif clean_prm==3:\n            X_less_toxic = df_val.apply(lambda row :clean3(row[\"less_toxic\"]),axis=1)\n            X_more_toxic = df_val.apply(lambda row :clean3(row[\"more_toxic\"]),axis=1)\n            X_test = df_test.apply(lambda row :clean3(row[\"text\"]),axis=1)\n\n        else:\n            X_less_toxic = df_val['less_toxic']\n            X_more_toxic = df_val['more_toxic']\n            X_test = df_test['text']\n        if fast_vec==True:\n            X_less_toxic_f=text2fasttextarray(X_less_toxic)\n            X_more_toxic_f=text2fasttextarray(X_more_toxic)\n            X_test_f=text2fasttextarray(X_test)\n            \n            val_preds_arr1[:,fld]=model_f.predict(X_less_toxic_f)*fast_factor\n            val_preds_arr2[:,fld]=model_f.predict(X_more_toxic_f)*fast_factor\n            test_preds_arr[:,fld]=model_f.predict(X_test_f)*fast_factor\n#             joblib.dump(model,os.path.join(ridge_save_path,f'{model_pre}_f_{fld}.pkl')) #保存模型 文件后缀为.pkl\n            \n        X_less_toxic = vec.transform(X_less_toxic)\n        X_more_toxic = vec.transform(X_more_toxic)\n        X_test = vec.transform(X_test)\n        \n            \n        val_preds_arr1[:,fld] += model.predict(X_less_toxic)\n        val_preds_arr2[:,fld] += model.predict(X_more_toxic)\n\n        test_preds_arr[:,fld] += model.predict(X_test)\n            \n#         joblib.dump(model,os.path.join(ridge_save_path,f'{model_pre}{fld}.pkl')) #保存模型 文件后缀为.pkl\n        del model,vec\n        \n    p1=val_preds_arr1.mean(axis=1)\n    p2=val_preds_arr2.mean(axis=1)\n    pv=test_preds_arr.mean(axis=1)\n    print(f'Validation Accuracy is { np.round((p1 < p2).mean() * 100,2)}')        \n    return p1,p2,pv","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:18:20.007651Z","iopub.status.idle":"2022-02-01T10:18:20.008159Z","shell.execute_reply.started":"2022-02-01T10:18:20.007906Z","shell.execute_reply":"2022-02-01T10:18:20.007944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lightgbm_cv(df_dic,n_folds,model_pre=\"jc_k_gbm_\",df_val=df_val,clean_prm=False,translate=False):\n    return 0","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:18:20.009799Z","iopub.status.idle":"2022-02-01T10:18:20.010278Z","shell.execute_reply.started":"2022-02-01T10:18:20.010049Z","shell.execute_reply":"2022-02-01T10:18:20.010072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p1=defaultdict()\np2=defaultdict()\npv=defaultdict()\n\nval_data=df_val\n\nfunc_dict={\"ridge\":ridge_cv,\"gbm\":lightgbm_cv}\n# func_dict.get(x)\n\nif Fold_type==2:\n    pre_names=[ data_name+\"s_\" for data_name in data_names]\n    name2dict={\"jc_s_\":jc_df_sdict,\"rud_s_\":rud_df_sdict,\n               \"jcc1_s_\":jcc1_df_sdict,\"jcc2_s_\":jcc2_df_sdict}\n    fold_num=fold_num_s\n\np1_ensenmble = np.zeros((val_data.shape[0]))\np2_ensenmble = np.zeros((val_data.shape[0]))\nscore=np.zeros((df_test.shape[0]))\n\nfor d_index,pre_name in enumerate(pre_names):\n    ###model_pre_ridge:jc_s_ridge_ pre_name:jc_s_ model:jc_s_ridge_{fold} vec:jc_s_ridge_vec_{fold}\n    #pre_name jc_s_ model_name jc_s_ridge_\n    clean_prm=False\n    translate=False\n    \n    p1[pre_name],p2[pre_name]=np.zeros((val_data.shape[0])),np.zeros((val_data.shape[0]))\n    pv[pre_name]=np.zeros((df_test.shape[0]))\n    if pre_name in clean_data.keys():\n        clean_prm=clean_data[pre_name]\n\n    for index,model_name in enumerate(model_choice):\n        cv_func=func_dict.get(model_name)\n        model_pre=pre_name+model_name+\"_\"\n        p1[model_pre],p2[model_pre],pv[model_pre]=cv_func(name2dict[pre_name],n_folds=fold_num,df_val=val_data,\n                                                        model_pre=model_pre,clean_prm=clean_prm,translate=translate)\n\n        p1[pre_name]= p1[pre_name]+ p1[model_pre]*factor[index]\n        p2[pre_name]= p2[pre_name]+ p2[model_pre]*factor[index]\n        pv[pre_name]= pv[pre_name]+ pv[model_pre]*factor[index]\n\n    kmax=max(p1[pre_name].max(),p2[pre_name].max())\n    result=df_val.copy()\n    result[\"less_value\"]=p1[pre_name]\n    result[\"more_value\"]=p2[pre_name]\n    result.to_csv(f\"{pre_name}.csv\")\n    p1_ensenmble=p1_ensenmble+factor_data[d_index]*p1[pre_name]/kmax\n    p2_ensenmble=p2_ensenmble+factor_data[d_index]*p2[pre_name]/kmax\n    score=score+factor_data[d_index]*pv[pre_name]/kmax\n\nprint(f' Validation Accuracy is { np.round((p1_ensenmble < p2_ensenmble).mean() * 100,4)}') \npredictions2=score","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:18:20.016330Z","iopub.execute_input":"2022-02-01T10:18:20.016576Z","iopub.status.idle":"2022-02-01T10:18:20.038494Z","shell.execute_reply.started":"2022-02-01T10:18:20.016545Z","shell.execute_reply":"2022-02-01T10:18:20.037383Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# ENSENMBLE","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn import preprocessing\npredictions1=preprocessing.scale(predictions1)\npredictions2=preprocessing.scale(predictions2)\n\n# predictions1 = (predictions1-predictions1.min())/(predictions1.max()-predictions1.min())\n# predictions2=(predictions2-predictions2.min())/(predictions2.max()-predictions2.min())","metadata":{"execution":{"iopub.status.busy":"2022-02-01T08:28:58.093748Z","iopub.execute_input":"2022-02-01T08:28:58.094483Z","iopub.status.idle":"2022-02-01T08:28:58.101872Z","shell.execute_reply.started":"2022-02-01T08:28:58.094443Z","shell.execute_reply":"2022-02-01T08:28:58.101028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions=0.67*predictions1+0.4*predictions2","metadata":{"execution":{"iopub.status.busy":"2022-02-01T08:29:01.79814Z","iopub.execute_input":"2022-02-01T08:29:01.798977Z","iopub.status.idle":"2022-02-01T08:29:01.80283Z","shell.execute_reply.started":"2022-02-01T08:29:01.798939Z","shell.execute_reply":"2022-02-01T08:29:01.802149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# submission","metadata":{}},{"cell_type":"code","source":"submission['score'] = predictions \nsubmission['score'] = submission['score'].rank(method='first')\nsubmission[['comment_id', 'score']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T08:30:17.776748Z","iopub.execute_input":"2022-02-01T08:30:17.777254Z","iopub.status.idle":"2022-02-01T08:30:17.806291Z","shell.execute_reply.started":"2022-02-01T08:30:17.777216Z","shell.execute_reply":"2022-02-01T08:30:17.805592Z"},"trusted":true},"execution_count":null,"outputs":[]}]}