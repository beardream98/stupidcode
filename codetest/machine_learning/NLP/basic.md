# 
## Normalization
Normlization归一化：一般用于消除量纲，在机器学习中假定各个维度服从正态分布，通过减去期望除以标准差，将其统一到参数相同的正态分布上。
归一化的优点：
1. 加快收敛速度，减少震荡(可以详细进一步讨论为什么)
2. 避免值本身，在进行距离计算时，主导了整个计算值
3. 避免数值计算问题
深度学习中的归一化主要包含两个方面：BN(Batch Normalization)和LN(Layer Normalization)。假定训练中输入矩阵维度为(N,S,H)
### BN
BN操作分为两个步骤：1.归一化 2.线性映射。其中线性映射时选择两个可以$\alpha$和$\beta$来映射。
$$y = \alpha x + \beta  $$
防止归一化对特征表达带来的破坏。即模型可以通过这两个参数映射回原来的值。当$\alpha$为标准差，$\beta$为期望时，自然变为了归一化前的值。
BN具体进行归一化操作时，对H维度上每一维进行归一化，即每一维上计算(N,S)构成的所有值的均值和方差。
**优点**：
1. 改善了ICS问题，即在参数更新情况下隐状态的分布产生了变化，而模型需要不断的去学习变化的分布。通过BN来固定隐层节点的输入。来源：Accelerating deep network training by reducing internal covariate shift 通过减少ICS来加速训练
2. 光滑了损失平面
> 扩展：当损失函数梯度小于一个常量时，满足Lipschitz连续，能使得损失函数平面光滑。加速收敛

***训练和测试阶段***
训练阶段好理解，当测试阶段时一般选用训练阶段的均值和方差，由于存在多个batch，可以取这两个参数的均值。
开源框架的具体实现：一般来说，开源框架会在训练阶段就通过滑动平均来收集这部分值，一个简单的想法时总batch为$m$个，每计算一次batch都将结果除$m$后，再求和。
>Keras:keras是使用了一个遗忘因子$t$,$E_{mov} = m*E_{old} + (1-m) * E_{new}  $ 

### LN
LN具体进行归一化操作时，对B维度上每一维进行归一化，即每一维上计算(S,H)构成的所有值的均值和方差。
NLP中一般都使用LN，因为文本的输入是不等长的，当使用BN时，(N,S)构成的矩阵中有许多空值，在batch较小情况下，均值和方差的抖动比较大。