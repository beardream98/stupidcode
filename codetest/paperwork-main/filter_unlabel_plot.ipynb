{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-05 21:27:59.943384: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from email.errors import StartBoundaryNotFoundDefect\n",
    "from pickle import TRUE\n",
    "from re import I\n",
    "from tkinter import ttk\n",
    "import yaml\n",
    "import data\n",
    "from data.read_data import select_data,TrainData,file_split,DataByFile,DataByWord\n",
    "from data.data_split import kfsplit\n",
    "from data.datasets import prepare_loaders\n",
    "from utils.wandb_init import get_run,wandb_utils\n",
    "from utils.train import get_score,train_set,get_score_list\n",
    "from utils.trivial import get_logger\n",
    "from model.head import OriginModel,get_model\n",
    "from model.trainer import train,predict\n",
    "import torch\n",
    "from transformers import logging\n",
    "from data.read_data import read_data_file\n",
    "\n",
    "import math\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "import gc \n",
    "import torch.nn.functional as F\n",
    "gc.enable()\n",
    "from utils.trivial import set_seed\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "import torch.distributed as dist\n",
    "from data.datasets import make_dataloader,prepare_loaders\n",
    "from utils.train import get_score,train_set,reduce_tensor,get_score_list,gather_tensor,gather_object\n",
    "from utils.train import remove_module\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def predict(df,DEVICE,CONFIG,model_path=None,model=None):\n",
    "    if model==None:\n",
    "        CONFIG[\"Logger\"].info(f\"\\nUsing{model_path}\")\n",
    "        model=get_model(CONFIG[\"model_name\"])(CONFIG)\n",
    "        model.to(DEVICE,non_blocking=True)\n",
    "        if CONFIG[\"DIST_MODEL\"]:\n",
    "            args=CONFIG[\"args\"]\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\\\n",
    "                output_device=args.local_rank,find_unused_parameters=True)\n",
    "            dist.barrier()\n",
    "        ### may cause error \n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "        else:\n",
    "            state_dict=remove_module(torch.load(model_path))\n",
    "            model.load_state_dict(state_dict)\n",
    "        \n",
    "    model.eval()\n",
    "    #set is_test=True to get label value\n",
    "    data_loader,_=make_dataloader(df,CONFIG[\"DEV_BATCH_SIZE\"],CONFIG[\"model_name\"],\\\n",
    "        CONFIG[\"MAX_LENGTH\"],is_test=False,is_dist=CONFIG[\"DIST_MODEL\"])\n",
    "\n",
    "    predict_list,index_list=[],[]\n",
    "\n",
    "    \n",
    "    for batch in tqdm(data_loader):\n",
    "        text_inputs=batch[\"text\"]\n",
    "        text_inputs={key: value.to(DEVICE,non_blocking=True) for key,value in text_inputs.items()}\n",
    "        index=batch[\"index\"]\n",
    "        with torch.no_grad():\n",
    "            preds=model(**text_inputs)\n",
    "            preds=F.softmax(preds,dim=1)\n",
    "        predict_list+=preds.cpu().detach().numpy().tolist()\n",
    "        index_list+=index.cpu().detach().numpy().tolist()\n",
    "    \n",
    "    assert(len(predict_list)==len(index_list))\n",
    "    \n",
    "    if CONFIG[\"DIST_MODEL\"] :\n",
    "        gather_array=np.concatenate([np.asarray(index_list).reshape(-1,1),predict_list],axis=1)\n",
    "        gather_list=gather_object(gather_array,world_size=CONFIG[\"world_size\"])\n",
    "        index_array,predict_array=np.split(np.concatenate(gather_list,axis=0),[1],axis=1)\n",
    "    else:\n",
    "        index_array,predict_array=np.array(index_list),np.array(predict_list)\n",
    "\n",
    "    \n",
    "    predictions=np.argmax(predict_array,axis=1)\n",
    "    '''\n",
    "    多卡情况下，当数据不能被均等分配给各卡时，例如2卡，余1份数据，\n",
    "    会将两份数据都分给两块卡，因此不用assert prediction与df大小\n",
    "    '''\n",
    "    # assert(len(predictions)==df.shape[0])\n",
    "\n",
    "    index_array.astype(int)\n",
    "\n",
    "    if index_array.ndim==2:\n",
    "        index_array=np.squeeze(index_array)\n",
    "    if predictions.ndim==2:\n",
    "        predictions=np.squeeze(predictions)\n",
    "\n",
    "    df[\"predict\"]=-1\n",
    "    \n",
    "    df.loc[index_array,\"prob0\"]=predict_array[:,0]\n",
    "    df.loc[index_array,\"prob1\"]=predict_array[:,1]\n",
    "\n",
    "    df.loc[index_array,\"predict\"]=predictions\n",
    "\n",
    "    if CONFIG[\"DIST_MODEL\"]:\n",
    "        dist.barrier()\n",
    "    ## may cause error \n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "def filter_floop(CONFIG,trainDataObj,DEVICE):\n",
    "\n",
    "    return \n",
    "\n",
    "def iter(select_word):\n",
    "    \n",
    "    file_path = os.path.join(CONFIG[\"INPUT_DIR\"],select_word)\n",
    "    positive_file = os.path.join(file_path,\"UnlabelData\")\n",
    "\n",
    "    positive_df = read_data_file(positive_file,\\\n",
    "        CONFIG[\"DEBUG_MODEL\"],data_num=CONFIG[\"DATA_NUM\"],head=column_name)\n",
    "    \n",
    "    positive_df[\"fold\"] = 1\n",
    "    positive_df[\"label\"] = positive_df[\"label\"].replace([\"0\",\"1\"],[0,1])\n",
    "    positive_df = positive_df.query(\"label==1 or label==0\")\n",
    "\n",
    "    return positive_df\n",
    "\n",
    "def get_binary_list(CONFIG):\n",
    "\n",
    "    binary_word_path = os.path.join(os.path.dirname(CONFIG[\"INPUT_DIR\"]),\"classifyword.txt\")\n",
    "    binary_word_list = []\n",
    "    with open(binary_word_path,\"r\") as fin:\n",
    "        for line in fin:\n",
    "            binary_word_list.append(line.strip())\n",
    "    return binary_word_list\n",
    "\n",
    "def get_swap_id(CONFIG):\n",
    "    swap_id = []\n",
    "    swap_id_path = os.path.join(os.path.dirname(CONFIG[\"INPUT_DIR\"]),\"swap_id\")\n",
    "    with open(swap_id_path,\"r\") as fin:\n",
    "        for line in fin:\n",
    "            swap_id.append(int(line.strip()))\n",
    "    return swap_id\n",
    "\n",
    "def get_unlabel_data(column_name,binary_word_list):\n",
    "    data_df = pd.DataFrame(columns = column_name)\n",
    "    for word in binary_word_list:\n",
    "        temp_df = iter(select_word = word)\n",
    "        data_df = pd.concat([data_df,temp_df],axis=0)\n",
    "    data_df = data_df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "def get_train_count(CONFIG,column_name,binary_word_list):\n",
    "    CONFIG[\"PSEUDO_DATA\"] = False\n",
    "    train_data_count_path = os.path.join(CONFIG[\"INPUT_DIR\"],\"trainDataCount\")\n",
    "    if not os.path.exists(train_data_count_path):\n",
    "        trainDataObj=DataByWord(CONFIG,\"train\",column_name=column_name)\n",
    "        trainData=trainDataObj.get_alldata()\n",
    "        train_word_num = {}\n",
    "        for word in binary_word_list:\n",
    "            positive_num = trainData.query(\"keyword == @word & label == 1\")\n",
    "            negative_num = trainData.query(\"keyword == @word & label == 0\")\n",
    "            train_word_num[word+\"_positive\"] = positive_num\n",
    "            train_word_num[word+\"_negative\"] = negative_num\n",
    "            \n",
    "        torch.save(train_word_num,train_data_count_path)\n",
    "    else:\n",
    "        train_word_num = torch.load(train_data_count_path)\n",
    "\n",
    "    return train_word_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_path = \"./config/train.yaml\"\n",
    "CONFIG,Logger,DEVICE = train_set(yaml_path,experimentName = None,upload = False,filename = \"./test/logs\",is_notebook = True)\n",
    "\n",
    "CONFIG[\"DEBUG_MODEL\"] = False\n",
    "CONFIG[\"run_db\"] = False\n",
    "CONFIG[\"SAVE_PATH\"] = \"/root/autodl-tmp/epsave/checkpoint/best/binarybert\"\n",
    "set_seed(CONFIG['SEED'])\n",
    "\n",
    "with open(os.path.join(CONFIG[\"INPUT_DIR\"],\"Schema\"),\"r\") as f:\n",
    "    line = f.readlines()[0].strip()\n",
    "    column_name = line.split(\"\\t\")\n",
    "    CONFIG[\"Logger\"].info(f\"column_name :{column_name}\")\n",
    "CONFIG[\"COLUMN_NAME\"] = column_name\n",
    "\n",
    "binary_word_list = get_binary_list(CONFIG)\n",
    "swap_id = get_swap_id(CONFIG)\n",
    "data_df = get_unlabel_data(column_name,binary_word_list)\n",
    "train_word_num = get_train_count(CONFIG,column_name,binary_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_num = min([value for value in train_word_num.values()])\n",
    "fold = 0 \n",
    "model_path=os.path.join(CONFIG[\"SAVE_PATH\"],f\"bestmodel{fold}.pth\")\n",
    "CONFIG[\"Logger\"].info(f\"\\nUsing{model_path}\")\n",
    "model=get_model(CONFIG[\"model_name\"])(CONFIG)\n",
    "model.to(DEVICE,non_blocking=True)\n",
    "if CONFIG[\"DIST_MODEL\"]:\n",
    "    args=CONFIG[\"args\"]\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\\\n",
    "        output_device=args.local_rank,find_unused_parameters=True)\n",
    "    dist.barrier()\n",
    "### may cause error \n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "else:\n",
    "    state_dict=remove_module(torch.load(model_path))\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "output_df = predict(data_df,DEVICE,CONFIG,model_path,model)\n",
    "\n",
    "unlabel_word_count = data_df[\"keyword\"].value_counts().to_dict()\n",
    "min_num = min(unlabel_word_count.values())\n",
    "\n",
    "for word in tqdm(binary_word_list):\n",
    "    word_file = os.path.join(CONFIG[\"INPUT_DIR\"],word,\"PseudoData\")\n",
    "    word_df = output_df.query(\"keyword == @word\")\n",
    "    positive_df = word_df.query(\"predict == 1\")\n",
    "    negative_df = word_df.query(\"predict == 0\")\n",
    "\n",
    "    positive_num,negative_num= train_word_num[word+\"_positive\"], train_word_num[word+\"_negative\"]\n",
    "    min_binary_num = min(positive_num,negative_num)\n",
    "\n",
    "    positive_sample_rate = ( math.sqrt(min_num/unlabel_word_count[word]) + math.sqrt(min_binary_num / train_word_num[word+\"_positive\"]) ) / 2 \n",
    "    positive_sample_rate = positive_sample_rate ** CONFIG[\"SAMPLE_ALPHA\"]\n",
    "\n",
    "    negative_sample_rate = ( math.sqrt(min_num/unlabel_word_count[word]) + math.sqrt(min_binary_num/train_word_num[word+\"_negative\"]) ) / 2 \n",
    "    negative_sample_rate = negative_sample_rate ** CONFIG[\"SAMPLE_ALPHA\"]\n",
    "\n",
    "    positive_sample_df = positive_df.sample(frac = positive_sample_rate)\n",
    "    negative_sample_df = negative_df.sample(frac = negative_sample_rate)\n",
    "\n",
    "    sample_df = pd.concat([positive_sample_df,negative_sample_df],axis = 0)\n",
    "    sample_df[\"label\"] = sample_df[\"predict\"]\n",
    "    if swap_id[binary_word_list.index(word)] == 1 :\n",
    "        sample_df[\"label\"].replace([0,1],[1,0])\n",
    "    sample_df[\"label\"] = sample_df[\"label\"].replace([0,1],[\"0\",\"1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
