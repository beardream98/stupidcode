{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import data\n",
    "from data.data_split import kfsplit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-04 23:33:13.018534: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import torch\n",
    "\n",
    "from torch.cuda.amp import autocast as autocast, GradScaler\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import gc \n",
    "gc.enable()\n",
    "from data.datasets import prepare_loaders\n",
    "\n",
    "import wandb\n",
    "from model.head import OriginModel,LR,get_model\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from model.trainer import train,predict\n",
    "from utils.wandb_init import get_run,wandb_utils\n",
    "from utils.train import get_score,train_set\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "from data.read_data import select_data,select_data,TrainData,file_split,DataByWord\n",
    "from data.data_split import kfsplit\n",
    "from data.embedding_init import get_tokenizer,get_embedding\n",
    "from data.datasets import DatasetRetriever\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madreambear\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "yaml_path = \"./config/train.yaml\"\n",
    "CONFIG,Logger,DEVICE = train_set(yaml_path,experimentName = None,upload = False,filename = \"./test/logs\",is_notebook = True)\n",
    "\n",
    "CONFIG[\"run_db\"] = False\n",
    "# CONFIG[\"num_hidden_layers\"] = 12\n",
    "CONFIG[\"DATA_NUM_RATE\"] = 0.2\n",
    "# CONFIG[\"EPOCHS\"] = 5\n",
    "CONFIG[\"DEBUG_MODEL\"] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "column_name :['Query', 'keyword', 'query_len', 'emotion_value', 'label']\n",
      "----------trainData label's value_counts----------\n",
      "0    190964\n",
      "1    174230\n",
      "Name: label, dtype: int64\n",
      "----------trainData fold's value_counts----------\n",
      "1.0    263965\n",
      "0.0    101229\n",
      "Name: fold, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(CONFIG[\"INPUT_DIR\"],\"Schema\"),\"r\") as f:\n",
    "    line=f.readlines()[0].strip()\n",
    "    column_name=line.split(\"\\t\")\n",
    "    CONFIG[\"Logger\"].info(f\"column_name :{column_name}\")\n",
    "    CONFIG[\"COLUMN_NAME\"]=column_name\n",
    "    \n",
    "if CONFIG[\"HARD_NEG\"]:\n",
    "    filter_rate,hard_rate=CONFIG[\"FILTER_RATE\"],CONFIG[\"HARD_RATE\"]\n",
    "    CONFIG[\"Logger\"].info(\"-\"*10+f\"filter_rate: {filter_rate} hard_rate: {hard_rate}\"+\"-\"*10)\n",
    "    trainDataObj=DataByWord(CONFIG,\"train\",column_name=column_name,filter_rate=filter_rate,hard_rate=hard_rate)\n",
    "else:\n",
    "    trainDataObj=DataByWord(CONFIG,\"train\",column_name=column_name)\n",
    "\n",
    "\n",
    "trainData=trainDataObj.get_alldata()\n",
    "# trainData=trainDataObj.get_data(select_word=\"coffee\")\n",
    "\n",
    "CONFIG[\"Logger\"].info(\"-\"*10+\"trainData label's value_counts\"+\"-\"*10)\n",
    "CONFIG[\"Logger\"].info(trainData[\"label\"].value_counts())\n",
    "CONFIG[\"Logger\"].info(\"-\"*10+\"trainData fold's value_counts\"+\"-\"*10)\n",
    "CONFIG[\"Logger\"].info(trainData[\"fold\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['candy',\n",
       " 'beans',\n",
       " 'go',\n",
       " 'superman',\n",
       " 'coffee',\n",
       " 'phoenix',\n",
       " 'tabs',\n",
       " 'live',\n",
       " 'bars',\n",
       " 'diesel',\n",
       " 'truck drivers',\n",
       " 'food',\n",
       " 'fire',\n",
       " 'girl',\n",
       " 'white',\n",
       " 'wet',\n",
       " 'zip',\n",
       " 'junk',\n",
       " 'leaf',\n",
       " 'shoes',\n",
       " 'flour',\n",
       " 'snowflake',\n",
       " 'green',\n",
       " 'space',\n",
       " 'trees',\n",
       " 'paint',\n",
       " 'lemonade',\n",
       " 'work',\n",
       " 'k',\n",
       " 'purple',\n",
       " 'poison',\n",
       " 'black',\n",
       " 'blue',\n",
       " 'salt',\n",
       " 'jet',\n",
       " 'ace',\n",
       " 'buttons',\n",
       " 'whites',\n",
       " 'him',\n",
       " 'bananas',\n",
       " 'blues',\n",
       " 'rope',\n",
       " 'mother',\n",
       " 'scratch',\n",
       " 'mph',\n",
       " 'water',\n",
       " 'speed']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataObj.binary_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data.read_data import read_data_file\n",
    "\n",
    "# i = trainDataObj.binary_word_list.index(select_word)\n",
    "# file_path = os.path.join(CONFIG[\"INPUT_DIR\"],trainDataObj.binary_word_list[i])\n",
    "# positive_file = os.path.join(file_path,\"Positive\"+trainDataObj.data_type.capitalize())\n",
    "# negative_file = os.path.join(file_path,\"Negative\"+trainDataObj.data_type.capitalize())\n",
    "# positive_df = read_data_file(positive_file,\\\n",
    "#     CONFIG[\"DEBUG_MODEL\"],data_num=CONFIG[\"DATA_NUM\"],head=trainDataObj.column_name)\n",
    "\n",
    "# negative_df = read_data_file(negative_file,\\\n",
    "#     CONFIG[\"DEBUG_MODEL\"],data_num=CONFIG[\"DATA_NUM\"],head=trainDataObj.column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df=trainData\n",
    "fold=0\n",
    "run=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------fold 0----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get model : bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------train start----------\n",
      "epoch 0 learning rate start  :0.0002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/516 [00:03<12:34,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:47<00:00,  2.10it/s]  \n",
      "EPOCH:1/10 train_loss:0.2672803997993469,val_loss:0.295006536173098\n",
      "f1:0.9010195158937888,recall:0.9009671141668889,precesion:0.9026284338285702,val_acc:0.9009671141668889\n",
      "one_epoch_time:4.471045692761739 min\n",
      "best_model saved ,f1:0.9010195158937888,recall:0.9009671141668889,precesion:0.9026284338285702,val acc:0.9009671141668889\n",
      "epoch 1 learning rate start  :0.00018\n",
      "100%|██████████| 99/99 [00:46<00:00,  2.14it/s]  \n",
      "EPOCH:2/10 train_loss:0.17958778301759284,val_loss:0.4279932641621792\n",
      "f1:0.8984145897782222,recall:0.8983591658516829,precesion:0.8991622359553456,val_acc:0.8983591658516829\n",
      "one_epoch_time:4.44686484336853 min\n",
      "epoch 2 learning rate start  :0.00016\n",
      " 22%|██▏       | 114/516 [00:49<02:50,  2.36it/s]"
     ]
    }
   ],
   "source": [
    "for fold in range(CONFIG[\"NUM_FOLDS\"]):\n",
    "\n",
    "    if CONFIG[\"cross_val\"]==False and fold>=1:\n",
    "        break\n",
    "    CONFIG[\"Logger\"].info(f\"----------fold {fold}----------\")\n",
    "    run=get_run(CONFIG,\"Train\",fold)\n",
    "    model=get_model(CONFIG[\"model_name\"])(CONFIG)\n",
    "    # model=OriginModel(CONFIG)\n",
    "    train(model,data_df,fold,CONFIG,DEVICE,run=run,trainDataObj=trainDataObj)\n",
    "    \n",
    "    # _,_,acc_train_lastepoch=predict(data_df,DEVICE,CONFIG,model_path=None,model=model)\n",
    "    \n",
    "    # CONFIG[\"Logger\"].info(f\"train{fold} acc in last epoch:{acc_train_lastepoch}\")\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    if run!=None:\n",
    "        run.finish()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
