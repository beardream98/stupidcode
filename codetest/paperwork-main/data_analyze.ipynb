{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import data\n",
    "from data.data_split import kfsplit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 22:39:11.733609: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import torch\n",
    "\n",
    "from torch.cuda.amp import autocast as autocast, GradScaler\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import gc \n",
    "gc.enable()\n",
    "from data.datasets import prepare_loaders\n",
    "\n",
    "import wandb\n",
    "from model.head import OriginModel,LR,get_model\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from model.trainer import train,predict\n",
    "from utils.wandb_init import get_run,wandb_utils\n",
    "from utils.train import get_score,train_set\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "from data.read_data import select_data,select_data,TrainData,file_split,DataByFile,DataByFileHard\n",
    "from data.data_split import kfsplit\n",
    "from data.embedding_init import get_tokenizer,get_embedding\n",
    "from data.datasets import DatasetRetriever\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madreambear\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "yaml_path = \"./config/train.yaml\"\n",
    "CONFIG,Logger,DEVICE = train_set(yaml_path,experimentName = None,upload = False,filename = \"./test/logs\",is_notebook = True)\n",
    "\n",
    "CONFIG[\"run_db\"] = False\n",
    "CONFIG[\"num_hidden_layers\"] = 12\n",
    "CONFIG[\"DATA_NUM_RATE\"] = 0.2\n",
    "CONFIG[\"EPOCHS\"] = 35\n",
    "CONFIG[\"DEBUG_MODEL\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "column_name :['Query', 'keyword', 'query_len', 'emotion_value', 'label']\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(CONFIG[\"INPUT_DIR\"],\"Schema\"),\"r\") as f:\n",
    "    line = f.readlines()[0].strip()\n",
    "    column_name = line.split(\"\\t\")\n",
    "    CONFIG[\"Logger\"].info(f\"column_name :{column_name}\")\n",
    "    CONFIG[\"COLUMN_NAME\"] = column_name\n",
    "head = CONFIG[\"COLUMN_NAME\"]\n",
    "input_dir = CONFIG[\"INPUT_DIR\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.read_data import read_data_file\n",
    "\n",
    "positive_train_file = os.path.join(input_dir,\"PositiveTrain\")\n",
    "positive_test_file = os.path.join(input_dir,\"PositiveTest\")\n",
    "\n",
    "positive_train_df = read_data_file(positive_train_file,False,5000,head = head)\n",
    "positive_test_df = read_data_file(positive_test_file,False,5000,head = head)\n",
    "positive_train_df[\"label\"],positive_test_df[\"label\"] = positive_train_df[\"label\"].astype(int),positive_test_df[\"label\"].astype(int)\n",
    "\n",
    "# read negative data\n",
    "negative_train_file = os.path.join(input_dir,\"NegativeTrain\")\n",
    "negative_test_file = os.path.join(input_dir,\"NegativeTest\")\n",
    "\n",
    "negative_train_df = read_data_file(negative_train_file,False,5000,head = head)\n",
    "negative_test_df = read_data_file(negative_test_file,False,5000,head = head)\n",
    "negative_train_df[\"label\"],negative_test_df[\"label\"] = negative_train_df[\"label\"].astype(int),negative_test_df[\"label\"].astype(int)\n",
    "\n",
    "# train_df = pd.concat([positive_train_df,negative_train_df],axis = 0)\n",
    "# test_df = pd.concat([positive_test_df,negative_test_df],axis = 0)\n",
    "\n",
    "# all_text = pd.concat([train_df,test_df],axis = 0)[\"Query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Columns must be same length as key",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_93366/1355794183.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnegative_train_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Query\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative_train_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"[MASK]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnegative_keyword_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnegative_train_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"keyword\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3643\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3644\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3645\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item_frame_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3646\u001b[0m         elif (\n\u001b[1;32m   3647\u001b[0m             \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item_frame_value\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3773\u001b[0m             \u001b[0mlen_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3774\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen_cols\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3775\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Columns must be same length as key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3777\u001b[0m             \u001b[0;31m# align right-hand-side columns if self.columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Columns must be same length as key"
     ]
    }
   ],
   "source": [
    "negative_train_df[\"Query\"]=negative_train_df.apply(lambda row:row[0].replace(row[1],\"[MASK]\"),axis=1)\n",
    "negative_keyword_counts = negative_train_df[\"keyword\"].value_counts().to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "appear_file=\"/root/autodl-tmp/epsave/data/raw_data/target_keywords_drug.txt\"\n",
    "appearword=[]\n",
    "with open(appear_file,\"r\",encoding=\"UTF-8\") as file_obj:\n",
    "    lines=file_obj.readlines()\n",
    "    for line in lines:\n",
    "        appearword.append(line.strip())\n",
    "\n",
    "keyword_file=\"/root/autodl-tmp/epsave/data/raw_data/euphemism_answer_drug.txt\"\n",
    "\n",
    "keyword_dict=defaultdict(list)\n",
    "with open(keyword_file,\"r\",encoding=\"UTF-8\") as file_obj:\n",
    "    for line in file_obj:\n",
    "        ans = line.split(':')[0].strip().lower()\n",
    "        for i in line.split(':')[1].split(';'):\n",
    "            keyword_dict[ans].append(i.strip().lower())\n",
    "\n",
    "keyword_file = \"/root/autodl-tmp/epsave/data/raw_data/euphemism_answer_drug.txt\"\n",
    "euphemism_answer = defaultdict(list)\n",
    "with open(keyword_file, 'r') as fin:\n",
    "    for line in fin:\n",
    "        ans = line.split(':')[0].strip().lower()\n",
    "        for i in line.split(':')[1].split(';'):\n",
    "            euphemism_answer[i.strip().lower()].append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "keyword_count = defaultdict(int)\n",
    "for key,word_list in keyword_dict.items():\n",
    "    for word in word_list:\n",
    "        keyword_count[word] +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'negative_keyword_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_93366/1683414587.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkeyword_count_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"keyword\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mkeyword_count_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeyword_count_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnegative_keyword_count_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"keyword\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnegative_keyword_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"negative_count\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnegative_keyword_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'negative_keyword_counts' is not defined"
     ]
    }
   ],
   "source": [
    "x,y = keyword_count.keys(),keyword_count.values()\n",
    "keyword_count_df = pd.DataFrame({\"keyword\":x,\"count\":y})\n",
    "keyword_count_df = keyword_count_df.sort_values(by='count',ascending=False)\n",
    "negative_keyword_count_df = pd.DataFrame({\"keyword\":negative_keyword_counts.keys(),\"negative_count\":negative_keyword_counts.values()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'negative_keyword_count_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_93366/182813835.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword_count_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnegative_keyword_count_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"keyword\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mall_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mall_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'negative_count'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'negative_keyword_count_df' is not defined"
     ]
    }
   ],
   "source": [
    "all_count = pd.merge(keyword_count_df,negative_keyword_count_df,on=\"keyword\",how=\"left\")\n",
    "all_count = all_count.fillna(value=0)\n",
    "all_count = all_count.sort_values(by=['negative_count','count'],ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/root/autodl-tmp/epsave/data/binary\"\n",
    "positive_file = os.path.join(input_dir,\"PositiveData.txt\")\n",
    "\n",
    "positive_df = read_data_file(positive_file,False,5000,head = head)\n",
    "positive_count_dict = positive_df[\"keyword\"].value_counts().to_dict()\n",
    "\n",
    "positive_keyword_count_dict = defaultdict(int)\n",
    "for key,count_num in positive_count_dict.items():\n",
    "    for word in keyword_dict[key]:\n",
    "        positive_keyword_count_dict[word] += count_num\n",
    "\n",
    "positive_count_df = pd.DataFrame({\"keyword\":positive_keyword_count_dict.keys(),\"positive_count\":positive_keyword_count_dict.values()})\n",
    "all_count = pd.merge(all_count,positive_count_df,on=\"keyword\",how=\"left\")\n",
    "all_count = all_count.fillna(value=0)\n",
    "all_count = all_count.sort_values(by=['negative_count','positive_count'],ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16549/3721869299.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exist_all_count[\"positive_rate\"] = exist_all_count[\"positive_count\"] / exist_all_count[\"negative_count\"]\n"
     ]
    }
   ],
   "source": [
    "exist_all_count = all_count.query(\"negative_count>100 & positive_count>100\")\n",
    "exist_all_count[\"positive_rate\"] = exist_all_count[\"positive_count\"] / exist_all_count[\"negative_count\"]\n",
    "exist_all_count = exist_all_count.sort_values(by=['positive_count','negative_count'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/root/autodl-tmp/epsave/data/binary\"\n",
    "unlabel_file = os.path.join(input_dir,\"unlabeldata.txt\")\n",
    "\n",
    "unlabel_df = read_data_file(unlabel_file,False,5000,head = [\"Query\",\"keyword\"])\n",
    "unlabel_df_count_dict = unlabel_df[\"keyword\"].value_counts().to_dict()\n",
    "unlabel_count_df = pd.DataFrame({\"keyword\":unlabel_df_count_dict.keys(),\"unlabel_count\":unlabel_df_count_dict.values()})\n",
    "unlalbe_all_count = pd.merge(exist_all_count,unlabel_count_df,on=\"keyword\",how=\"left\")\n",
    "unlalbe_all_count = unlalbe_all_count.fillna(value=0)\n",
    "unlalbe_all_count = unlalbe_all_count.sort_values(by='positive_count',ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47, 5)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exist_all_count_unique = exist_all_count.drop_duplicates(['positive_count'],keep='first')\n",
    "exist_all_count_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_filter_num = 47\n",
    "words_by_unlabel = unlalbe_all_count.query(\"unlabel_count>100\").iloc[:max_filter_num,0].values.tolist()\n",
    "words_by_negative = exist_all_count_unique.iloc[:max_filter_num,0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cover_positive_rate(words):\n",
    "    keyword_set = set()\n",
    "    for word in words:\n",
    "        for keyword in euphemism_answer[word]:\n",
    "            keyword_set.add(keyword)\n",
    "    cnt = 0\n",
    "    for keyword in keyword_set:\n",
    "        if keyword in positive_count_dict.keys():\n",
    "            cnt += positive_count_dict[keyword]\n",
    "    print(cnt)\n",
    "    positive_num = positive_df.shape[0]\n",
    "    print(cnt/positive_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4332\n",
      "0.8617465685299384\n"
     ]
    }
   ],
   "source": [
    "cover_positive_rate(words_by_unlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4931\n",
      "0.9809031231350707\n"
     ]
    }
   ],
   "source": [
    "cover_positive_rate(words_by_negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/root/autodl-tmp/epsave/data/binary/classifyword.txt\",\"w\") as write_object:\n",
    "        for word in words_by_negative:\n",
    "            write_object.write(word+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
