{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a5c9c3e",
   "metadata": {},
   "source": [
    "# 读取文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f60caa3c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_files_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-89b3e0d59eb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#读取json文件 返回text数据 添加到dataframe中\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mread_append_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_files_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_files_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \"\"\"\n\u001b[0;32m      4\u001b[0m     \u001b[0mFunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mread\u001b[0m \u001b[0mjson\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mthen\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mthem\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mappend\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \"\"\"\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_files_path' is not defined"
     ]
    }
   ],
   "source": [
    "#读取json文件 返回text数据 添加到dataframe中\n",
    "def read_append_return(filename, train_files_path, output='text'):\n",
    "    \"\"\"\n",
    "    Function to read json file and then return the text data from them and append to the dataframe\n",
    "    \"\"\"\n",
    "    json_path = os.path.join(train_files_path, (filename+'.json'))\n",
    "    headings = []\n",
    "    contents = []\n",
    "    combined = []\n",
    "    with open(json_path, 'r') as f:\n",
    "        json_decode = json.load(f)\n",
    "        for data in json_decode:\n",
    "            headings.append(data.get('section_title'))\n",
    "            contents.append(data.get('text'))\n",
    "            combined.append(data.get('section_title'))\n",
    "            combined.append(data.get('text'))\n",
    "    \n",
    "    all_headings = ' '.join(headings)\n",
    "    all_contents = ' '.join(contents)\n",
    "    all_data = '. '.join(combined)\n",
    "    \n",
    "    if output == 'text':\n",
    "        return all_contents\n",
    "    elif output == 'head':\n",
    "        return all_headings\n",
    "    else:\n",
    "        return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a93ed3",
   "metadata": {},
   "source": [
    "# progress_apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bac012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "df[column_name].progress_apply(function_name) #\n",
    "df[column_name].apply(function_name,axis,**kwds) #支持按行列进行，以及额外的输入补充 kwds需提供具体名"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc53d1b",
   "metadata": {},
   "source": [
    "# 最常出现的一百个词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除停用词\n",
    "words =list( train_df['cleaned_label'].values)\n",
    "stopwords=['ourselves', 'hers','the','of','and','in', 'between', 'yourself', 'but', 'again','of', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "split_words=[]\n",
    "for word in words:\n",
    "    lo_w=[]\n",
    "    list_of_words=str(word).split()\n",
    "    for w in list_of_words:\n",
    "        if w not in stopwords:\n",
    "            lo_w.append(w)\n",
    "    split_words.append(lo_w)\n",
    "#split_words 以句子为单位的列表\n",
    "allwords = []\n",
    "for wordlist in split_words:\n",
    "    allwords += wordlist\n",
    "    \n",
    "#最常出现的一百个词\n",
    "mostcommon = FreqDist(allwords).most_common(100)\n",
    "wordcloud = WordCloud(width=1600, height=800, background_color='white', stopwords=STOPWORDS).generate(str(mostcommon))\n",
    "fig = plt.figure(figsize=(30,10), facecolor='white')\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.title('Top 100 Most Common Words in cleaned_label', fontsize=50)\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()\n",
    "\n",
    "mostcommon_small = FreqDist(allwords).most_common(25)\n",
    "x, y = zip(*mostcommon_small)\n",
    "plt.figure(figsize=(50,30))\n",
    "plt.margins(0.02)\n",
    "plt.bar(x, y)\n",
    "plt.xlabel('Words', fontsize=50)\n",
    "plt.ylabel('Frequency of Words', fontsize=50)\n",
    "plt.yticks(fontsize=40)\n",
    "plt.xticks(rotation=60, fontsize=40)\n",
    "plt.tight_layout(pad=0)\n",
    "plt.title('Freq of 25 Most Common Words in cleaned_label', fontsize=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f5ffc2",
   "metadata": {},
   "source": [
    "# 文本查找"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabc7ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#查找标签出现的测试集的句子\n",
    "tag_semtence_all=[]\n",
    "for id in sample_sub['Id'].unique():\n",
    "    # paper\n",
    "    paper = papers_sample[id]\n",
    "    tag_sentence=[]\n",
    "    # sentences\n",
    "    sentences_sample = set([clean_training_text(sentence) for section in paper \n",
    "                 for sentence in section['text'].split('.') ])\n",
    "    for label in all_labels:\n",
    "        \n",
    "        tag_sentence+=[sentence for sentence in sentences_sample if label in clean_text(sentence)]\n",
    "    \n",
    "    tag_semtence_all.append(tag_sentence)\n",
    "tag_csv=pd.DataFrame({\"Id\":sample_sub['Id'],\"tag_sentence\":tag_semtence_all})\n",
    "tag_csv.to_csv('tag_sentence.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952a714f",
   "metadata": {},
   "source": [
    "# 数据ID 重复时的数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47934edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.groupby('Id').agg({\n",
    "    'pub_title': 'first',\n",
    "    'dataset_title': '|'.join,\n",
    "    'dataset_label': '|'.join,\n",
    "    'cleaned_label': '|'.join\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a2cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本压缩\n",
    "def shorten_sentences(sentences):\n",
    "    short_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        if len(words) > MAX_LENGTH:\n",
    "            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n",
    "                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n",
    "        else:\n",
    "            short_sentences.append(sentence)\n",
    "    return short_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195c84ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#打乱数据\n",
    "# shuffling\n",
    "random.shuffle(ner_data) #ner_data 是列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f512dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词对应标签扩充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2beaede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#由于wordpiece tag也需要相应的扩充\n",
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "\n",
    "        # Tokenize the word and count number of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels\n",
    "\n",
    "tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(sentences_train, nes_train)\n",
    "]\n",
    "\n",
    "sentences_train_toknized = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "\n",
    "nes_train_toknized = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07f651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#消除字符级别分词的问题\n",
    "for pred_tag,valid_tag,val_input in zip(pred_tags,valid_tags,val_inputs):\n",
    "    #不使用decode 是因为用decode可以消除## 但是会导致和pred_rags 的不吻合\n",
    "    tokens=tokenizer.convert_ids_to_tokens(np.array(val_input))\n",
    "    new_tokens, new_pred_labels ,new_true_tags= [], [],[]\n",
    "    for token, pred_word,valid_word in zip(tokens, pred_tag,valid_tag):\n",
    "\n",
    "        if token.startswith(\"##\"):\n",
    "            new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "        else:\n",
    "            new_pred_labels.append(pred_word)\n",
    "            new_true_tags.append(valid_word)\n",
    "            new_tokens.append(token)\n",
    "    new_pred_labels=[new_pred_labels]\n",
    "    new_true_tags=[new_true_tags]\n",
    "    new_tokens=[new_tokens]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python36] *",
   "language": "python",
   "name": "conda-env-python36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
