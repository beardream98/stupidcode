{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import glob\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 64 # max no. words for each sentence.\n",
    "OVERLAP = 20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n",
    "\n",
    "MAX_SAMPLE = None # set a small number for experimentation, set None for production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. raw training rows: 19661\n"
     ]
    }
   ],
   "source": [
    "train_path = './input/train.csv'\n",
    "paper_train_folder = './input/train'\n",
    "\n",
    "paper_sample_folder='./input/test'\n",
    "sample_sub = pd.read_csv('./input/sample_submission.csv')\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "train = train[:MAX_SAMPLE]\n",
    "print(f'No. raw training rows: {len(train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by publication, training labels should have the same form as expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. grouped training rows: 14316\n"
     ]
    }
   ],
   "source": [
    "train = train.groupby('Id').agg({\n",
    "    'pub_title': 'first',\n",
    "    'dataset_title': '|'.join,\n",
    "    'dataset_label': '|'.join,\n",
    "    'cleaned_label': '|'.join\n",
    "}).reset_index()\n",
    "\n",
    "print(f'No. grouped training rows: {len(train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_papers(paper_folder,data_df):\n",
    "    papers = {}\n",
    "    for paper_id in data_df['Id'].unique():\n",
    "        with open(f'{paper_folder}/{paper_id}.json', 'r') as f:\n",
    "            paper = json.load(f)\n",
    "            papers[paper_id] = paper\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform data to NER format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_training_text(txt):\n",
    "    \"\"\"\n",
    "    similar to the default clean_text function but without lowercasing.\n",
    "    \"\"\"\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n",
    "\n",
    "def shorten_sentences(sentences):\n",
    "    short_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        if len(words) > MAX_LENGTH:\n",
    "            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n",
    "                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n",
    "        else:\n",
    "            short_sentences.append(sentence)\n",
    "    return short_sentences\n",
    "\n",
    "def find_sublist(big_list, small_list):\n",
    "    all_positions = []\n",
    "    for i in range(len(big_list) - len(small_list) + 1):\n",
    "        if small_list == big_list[i:i+len(small_list)]:\n",
    "            all_positions.append(i)\n",
    "    \n",
    "    return all_positions\n",
    "\n",
    "def tag_sentence(sentence, labels): # requirement: both sentence and labels are already cleaned\n",
    "    sentence_words = sentence.split()  \n",
    "    if labels is not None and any(re.findall(f'\\\\b{label}\\\\b', sentence)\n",
    "                                  for label in labels): # positive sample\n",
    "        nes = ['O'] * len(sentence_words)\n",
    "        for label in labels:\n",
    "            label_words = label.split()\n",
    "\n",
    "            all_pos = find_sublist(sentence_words, label_words)\n",
    "            for pos in all_pos:\n",
    "                nes[pos] = 'B'\n",
    "                for i in range(pos+1, pos+len(label_words)):\n",
    "                    nes[i] = 'I'\n",
    "\n",
    "        return True, list(zip(sentence_words, nes))\n",
    "        \n",
    "    else: # negative sample\n",
    "        nes = ['O'] * len(sentence_words)\n",
    "        return False, list(zip(sentence_words, nes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training data size: 47202 positives + 514263 negatives: 100%|███████████████████| 14316/14316 [01:38<00:00, 163.48it/s]"
     ]
    }
   ],
   "source": [
    "#返回ner_data\n",
    "cnt_pos, cnt_neg = 0, 0 # number of sentences that contain/not contain labels\n",
    "ner_data = []\n",
    "papers_train=read_papers(paper_train_folder,train)\n",
    "pbar = tqdm(total=len(train))\n",
    "for i, id, dataset_label in train[['Id', 'dataset_label']].itertuples():\n",
    "    # paper\n",
    "    paper = papers_train[id]\n",
    "    \n",
    "    # labels\n",
    "    labels = dataset_label.split('|')\n",
    "    labels = [clean_training_text(label) for label in labels]\n",
    "    \n",
    "    # sentences\n",
    "    sentences = set([clean_training_text(sentence) for section in paper \n",
    "                 for sentence in section['text'].split('.') \n",
    "                ])\n",
    "    sentences = shorten_sentences(sentences) # make sentences short\n",
    "    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n",
    "    \n",
    "    # positive sample\n",
    "    for sentence in sentences:\n",
    "        is_positive, tags = tag_sentence(sentence, labels)\n",
    "        if is_positive:\n",
    "            cnt_pos += 1\n",
    "            ner_data.append(tags)\n",
    "        elif any(word in sentence.lower() for word in ['data', 'study']): \n",
    "            \n",
    "            #ner_data.append(tags)\n",
    "            cnt_neg += 1\n",
    "    \n",
    "    # process bar\n",
    "    pbar.update(1)\n",
    "    pbar.set_description(f\"Training data size: {cnt_pos} positives + {cnt_neg} negatives\")\n",
    "\n",
    "# shuffling\n",
    "random.shuffle(ner_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list,nes_list=[],[]\n",
    "for row in ner_data:\n",
    "    words, nes = list(zip(*row))\n",
    "    words_list.append(list(words))\n",
    "    nes_list.append(list(nes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#crf标注\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import CRF   # CRF的具体实现太过复杂，这里我们借助一个外部的库\n",
    "\n",
    "\n",
    "def word2features(sent, i,pos_sent):\n",
    "    \"\"\"抽取单个字的特征\"\"\"\n",
    "    word = sent[i]\n",
    "    postag = pos_sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        #提取单词后缀，比如ly结尾可能代表副词\n",
    "        \"word[-3:]\": word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        #提取前一个单词特征\n",
    "        word1 = sent[i-1]\n",
    "        postag1 = pos_sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        #提取后一个单词特征\n",
    "        word1 = sent[i+1]\n",
    "        postag1 = pos_sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    \"\"\"抽取序列特征\"\"\"\n",
    "    pos_sent=nltk.tag.pos_tag(sent)\n",
    "    return [word2features(sent, i,pos_sent) for i in range(len(sent))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training data size: 47202 positives + 514263 negatives: 100%|███████████████████| 14316/14316 [01:50<00:00, 163.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = [sent2features(s) for s in words_list]\n",
    "y_train = nes_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "papers_sample=read_papers(paper_sample_folder,sample_sub)\n",
    "sentences_sample={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alzheimer s disease neuroimaging initiative adni\n",
      "trends in international mathematics and science study|education major field of study|nces|common core of data|trends in international mathematics\n",
      "coastal change hazards|nc sea level rise risk management study|slosh safir|slosh meows|noaa storm surge inundation|las dataset data management in arcgis|slosh model\n",
      "rural urban continuum codes\n"
     ]
    }
   ],
   "source": [
    "datasets_all=[]\n",
    "def clean_text(txt):\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n",
    "for id in sample_sub['Id'].unique():\n",
    "    # paper\n",
    "    paper = papers_sample[id]\n",
    "    \n",
    "    # sentences\n",
    "    sentences_sample = set([clean_training_text(sentence) for section in paper \n",
    "                 for sentence in section['text'].split('.') ])\n",
    "    \n",
    "    sentences_sample = shorten_sentences(sentences_sample) # make sentences short\n",
    "    sentences_sample = [sentence for sentence in sentences_sample if len(sentence) > 10] \n",
    "    sentences_sample=[sentence.split(\" \") for sentence in sentences_sample]\n",
    "    features_sample=[sent2features(s) for s in sentences_sample]\n",
    "    \n",
    "    #预测并提取预测结果\n",
    "    y_pred = crf.predict(features_sample)\n",
    "    datasets=[]\n",
    "    for i  in range(len(y_pred)):\n",
    "        if \"B\" in y_pred[i]:\n",
    "            dataset=\"\"\n",
    "            for j in range(len(y_pred[i])):\n",
    "                if y_pred[i][j]==\"B\":\n",
    "                    if dataset!=\"\":\n",
    "                        dataset=dataset.rstrip()\n",
    "                        dataset=clean_text(dataset)\n",
    "                        if dataset  not in datasets:\n",
    "                            datasets.append(dataset)\n",
    "                        dataset=\"\"\n",
    "                    dataset+=sentences_sample[i][j]+\" \"\n",
    "                elif y_pred[i][j]==\"I\":\n",
    "                    dataset+=sentences_sample[i][j]+\" \"\n",
    "            if dataset!=\"\":\n",
    "                dataset=dataset.rstrip()\n",
    "                dataset=clean_text(dataset)\n",
    "                if dataset  not in datasets:\n",
    "                    datasets.append(dataset)\n",
    "    \n",
    "    datasets=\"|\".join(datasets)\n",
    "    datasets_all.append(datasets)\n",
    "    print(datasets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. different labels: 638\n"
     ]
    }
   ],
   "source": [
    "#string matchinf  return literal_preds \n",
    "all_labels = set()\n",
    "\n",
    "for label_1, label_2, label_3 in train[['dataset_title', 'dataset_label', 'cleaned_label']].itertuples(index=False):\n",
    "    all_labels.add(str(label_1).lower())\n",
    "    all_labels.add(str(label_2).lower())\n",
    "    all_labels.add(str(label_3).lower())\n",
    "    \n",
    "print(f'No. different labels: {len(all_labels)}')\n",
    "\n",
    "\n",
    "def totally_clean_text(txt):\n",
    "    txt = clean_text(txt)\n",
    "    txt = re.sub(' +', ' ', txt)\n",
    "    return txt\n",
    "literal_preds = []\n",
    "\n",
    "for paper_id in sample_sub['Id']:\n",
    "    paper = papers_sample[paper_id]\n",
    "    text_1 = '. '.join(section['text'] for section in paper).lower()\n",
    "    text_2 = totally_clean_text(text_1)\n",
    "    \n",
    "    labels = set()\n",
    "    for label in all_labels:\n",
    "        if label in text_1 or label in text_2:\n",
    "            labels.add(clean_text(label))\n",
    "    \n",
    "    literal_preds.append('|'.join(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(datasets_all)):\n",
    "    if literal_preds[i]!=\"\":\n",
    "        if datasets_all[i]==\"\":\n",
    "            datasets_all[i]=literal_preds[i]\n",
    "        else:\n",
    "            datasets_all[i]+=\"|\"+literal_preds[i]\n",
    "    datasets_all[i]=datasets_all[i].split(\"|\")\n",
    "    datasets_all[i]=list(set(datasets_all[i]))\n",
    "    datasets_all[i]=\"|\".join(datasets_all[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission=pd.DataFrame({\"Id\":sample_sub['Id'],\"PredictionString\":datasets_all})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alzheimer s disease neuroimaging initiative adni|adni',\n",
       " 'common core of data|nces|trends in international mathematics and science study|trends in international mathematics|education major field of study',\n",
       " 'nc sea level rise risk management study|noaa storm surge inundation|slosh meows|slosh model|las dataset data management in arcgis|slosh safir|coastal change hazards|sea lake and overland surges from hurricanes',\n",
       " 'rural urban continuum codes']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
