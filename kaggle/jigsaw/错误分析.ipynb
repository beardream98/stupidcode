{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "import time\n",
    "import string\n",
    "from collections import defaultdict, deque\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dir=r\"C:\\Users\\Lenovo\\Desktop\\stupidcode\\kaggle\\jigsaw\\error_analyze\"\n",
    "# aug1_paths=os.path.join(input_dir,f\"aug_1\\\\data_df_aug1\")\n",
    "aug1_paths=\"./cv_analyze/luke_ruddit_0.860.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_val=pd.read_csv(\"./cv_analyze/roberta_0.824.csv\")\n",
    "def generate_comments(data):\n",
    "    more_toxic_text=data[[\"more_toxic\",\"encode_more\",\"more_value\"]]\n",
    "    less_toxic_text=data[[\"less_toxic\",\"encode_less\",\"less_value\"]]\n",
    "    more_toxic_text=more_toxic_text.rename(columns={\"more_toxic\":\"text\",\"encode_more\":\"encode_text\",\"more_value\":\"toxic_value\"})\n",
    "    less_toxic_text=less_toxic_text.rename(columns={\"less_toxic\":\"text\",\"encode_less\":\"encode_text\",\"less_value\":\"toxic_value\"})\n",
    "    \n",
    "    comments=pd.concat([more_toxic_text,less_toxic_text],axis=0)\n",
    "    comments=comments.drop_duplicates(subset=None,keep=\"first\",inplace=False) \n",
    "    return comments\n",
    "def encode_label(data_df):\n",
    "     # 不同训练结果下的偏差，进行一些处理 主要是luke的偏差\n",
    "    comments=generate_comments(roberta_val)\n",
    "    comments.index=comments[\"text\"]\n",
    "    index_encode_dict=comments.to_dict()[\"encode_text\"]\n",
    "    del comments\n",
    "    # 选择直接赋值 而不再根据文本匹配 因为经过翻译后的结果可能不匹配\n",
    "    encode_less_luke=data_df.apply(lambda row:index_encode_dict[row[\"less_toxic\"]],axis=1)\n",
    "    encode_more_luke=data_df.apply(lambda row:index_encode_dict[row[\"more_toxic\"]],axis=1)\n",
    "    data_df[\"encode_less\"]=encode_less_luke\n",
    "    data_df[\"encode_more\"]=encode_more_luke\n",
    "    return data_df\n",
    "def sort_by_roberta(data_df):\n",
    "    data_df[\"new_index\"]=data_df.merge(roberta_val[[\"encode_less\",\"encode_more\",\"worker\",\"Unnamed: 0\"]],\n",
    "         on=[\"encode_less\",\"encode_more\",\"worker\"],how=\"left\")[\"Unnamed: 0\"]\n",
    "    data_df.index=data_df[\"new_index\"]\n",
    "    data_df.sort_index(inplace=True)\n",
    "    return data_df\n",
    "def luke_process(data_df):\n",
    "    data_df=encode_label(data_df)\n",
    "    data_df=data_df.rename(columns={\"less_toxic_pred\":\"less_value\",\"more_toxic_pred\":\"more_value\"})\n",
    "    data_df[\"pair_True\"]=data_df[\"less_value\"]<data_df[\"more_value\"]\n",
    "\n",
    "    data_df=sort_by_roberta(data_df)\n",
    "    return data_df\n",
    "\n",
    "data_df=pd.read_csv(aug1_paths)\n",
    "data_df=luke_process(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_rank=pd.read_csv(\"./cv_analyze/bt_work.csv\")\n",
    "comments_rank=comments_rank[[\"text\",\"score\"]]\n",
    "\n",
    "# comments_rank.index=comments_rank.text\n",
    "# text2rank=comments_rank.to_dict()[\"score\"]\n",
    "\n",
    "# data_df[\"less_value\"]=data_df.apply(lambda row:text2rank[row[\"less_toxic\"]],axis=1)\n",
    "# data_df[\"more_value\"]=data_df.apply(lambda row:text2rank[row[\"more_toxic\"]],axis=1)\n",
    "# data_df[\"pair_True\"]=data_df.less_value<data_df.more_value\n",
    "\n",
    "def generate_comments(data):\n",
    "    more_toxic_text=data[[\"more_toxic\",\"encode_more\",\"more_value\"]]\n",
    "    less_toxic_text=data[[\"less_toxic\",\"encode_less\",\"less_value\"]]\n",
    "    more_toxic_text=more_toxic_text.rename(columns={\"more_toxic\":\"text\",\"encode_more\":\"encode_text\",\"more_value\":\"toxic_value\"})\n",
    "    less_toxic_text=less_toxic_text.rename(columns={\"less_toxic\":\"text\",\"encode_less\":\"encode_text\",\"less_value\":\"toxic_value\"})\n",
    "    \n",
    "    comments=pd.concat([more_toxic_text,less_toxic_text],axis=0)\n",
    "    comments=comments.drop_duplicates(subset=None,keep=\"first\",inplace=False) \n",
    "    return comments\n",
    "comments=generate_comments(data_df)\n",
    "comments_rank=comments_rank.merge(comments[[\"text\",\"encode_text\"]],on=\"text\",how=\"left\")\n",
    "less_fold=comments_rank.merge(data_df[[\"encode_less\",\"fold\"]],left_on=\"encode_text\",right_on=\"encode_less\",\n",
    "                              how=\"left\").drop_duplicates(\"text\")[\"fold\"]\n",
    "more_fold=comments_rank.merge(data_df[[\"encode_more\",\"fold\"]],left_on=\"encode_text\",right_on=\"encode_more\",\n",
    "                              how=\"left\").drop_duplicates(\"text\")[\"fold\"]\n",
    "\n",
    "less_fold=less_fold.rename(\"less_fold\").reset_index(drop=True)\n",
    "more_fold=more_fold.rename(\"more_fold\").reset_index(drop=True)\n",
    "fold_df=pd.concat([less_fold,more_fold],axis=1)\n",
    "comments_rank[\"fold\"]=fold_df.apply(lambda row:row[\"more_fold\"] if pd.isna(row[\"less_fold\"]) else row[\"less_fold\"],axis=1)\n",
    "comments_rank[\"fold\"]=comments_rank.fold.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data_df=pd.read_csv(aug1_paths)\n",
    "data_df[\"label_min\"]=data_df.apply(lambda row:row[\"encode_less\"] if row[\"encode_less\"]<row[\"encode_more\"] else row[\"encode_more\"],axis=1)\n",
    "data_df[\"label_max\"]=data_df.apply(lambda row:row[\"encode_less\"] if row[\"encode_less\"]>row[\"encode_more\"] else row[\"encode_more\"],axis=1)\n",
    "data_df[\"label_min_text\"]=data_df.apply(lambda row:row[\"less_toxic\"] if row[\"encode_less\"]<row[\"encode_more\"] else row[\"more_toxic\"],axis=1)\n",
    "data_df[\"label_max_text\"]=data_df.apply(lambda row:row[\"less_toxic\"] if row[\"encode_less\"]>row[\"encode_more\"] else row[\"more_toxic\"],axis=1)\n",
    "data_df[\"label_min_value\"]=data_df.apply(lambda row:row[\"less_value\"] if row[\"encode_less\"]<row[\"encode_more\"] else row[\"more_value\"],axis=1)\n",
    "data_df[\"label_max_value\"]=data_df.apply(lambda row:row[\"less_value\"] if row[\"encode_less\"]>row[\"encode_more\"] else row[\"more_value\"],axis=1)\n",
    "\n",
    "data_df[\"win_min\"]=data_df.apply(lambda row:1 if row[\"label_min\"]==row[\"encode_more\"] else 0,axis=1)\n",
    "data_df[\"win_max\"]=data_df.apply(lambda row:0 if row[\"label_min\"]==row[\"encode_more\"] else 1,axis=1)\n",
    "\n",
    "## pair True 已经失效\n",
    "\n",
    "# data_df[\"pair_True\"].replace({True:1,False:-1},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin_cv: 0.6965258403082237\n",
      "aug_data_cv: nan\n"
     ]
    }
   ],
   "source": [
    "origin_data_df=data_df[data_df[\"worker\"]!=999]\n",
    "aug_data_df=data_df[data_df[\"worker\"]==999]\n",
    "\n",
    "print(\"origin_cv:\",origin_data_df[\"pair_True\"].mean())\n",
    "print(\"aug_data_cv:\",aug_data_df[\"pair_True\"].mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7434705184012663"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_data_df_agg=origin_data_df.groupby([\"label_min\",\"label_max\",\"label_min_value\",\"label_max_value\",\"label_min_text\",\"label_max_text\",\"fold\"\n",
    "                                          ]).agg({\"win_min\":\"sum\",\"win_max\":\"sum\"}).reset_index()\n",
    "origin_data_df_agg[\"pair_True\"]=((origin_data_df_agg.label_min_value-origin_data_df_agg.label_max_value)*(\n",
    "    origin_data_df_agg.win_min-origin_data_df_agg.win_max))>0\n",
    "\n",
    "origin_data_df_agg.pair_True.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sleep\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\sleep\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7466255302738141"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# origin_error_df=origin_data_df[origin_data_df[\"pair_True\"]==False]\n",
    "# aug_error_df=aug_data_df[aug_data_df[\"pair_True\"]==False]\n",
    "# #找到所有的false 组合的情况（包括被判断正确的组合）\n",
    "origin_error_df=origin_data_df_agg[origin_data_df_agg[\"pair_True\"]==False]\n",
    "\n",
    "## error\n",
    "origin_error_df[\"encode_less\"]=origin_error_df.apply(lambda row:row[\"label_min\"] \n",
    "                                             if row[\"win_min\"]<row[\"win_max\"] else row[\"label_max\"],axis=1)\n",
    "origin_error_df[\"encode_more\"]=origin_error_df.apply(lambda row:row[\"label_min\"] \n",
    "                                             if row[\"win_min\"]>row[\"win_max\"] else row[\"label_max\"],axis=1)\n",
    "\n",
    "# # 76%的错误都来自 毒性相似句子 所有句子对中 52%是毒性相似句子\n",
    "confuse_error_df=origin_error_df[(origin_error_df[\"win_min\"]!=0) & (origin_error_df[\"win_max\"]!=0)]\n",
    "len(confuse_error_df)/len(origin_error_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_error(fold_num,origin_error_df):\n",
    "    train=pd.DataFrame(columns={\"text\",\"score\",\"encode_text\",\"fold\",\"sample\"})\n",
    "    for fold in range(fold_num):\n",
    "        error_df_fold=origin_error_df[origin_error_df.fold==fold]\n",
    "        comments_rank_fold=comments_rank[comments_rank.fold==fold]\n",
    "        comments_rank_fold=comments_rank_fold.sort_values(\"score\").reset_index(drop=True)\n",
    "        temp_comments=comments_rank_fold.copy()\n",
    "        temp_comments.index=temp_comments.encode_text\n",
    "        encode2score=temp_comments.to_dict()[\"score\"]\n",
    "        del temp_comments\n",
    "        temp_comments=comments_rank_fold.copy()\n",
    "        temp_comments[\"old_index\"]=temp_comments.index\n",
    "        temp_comments.index=temp_comments.score\n",
    "        score2index=temp_comments.to_dict()[\"old_index\"]\n",
    "        del temp_comments\n",
    "        encode1=error_df_fold.encode_less.values\n",
    "        encode2=error_df_fold.encode_more.values\n",
    "        comments_rank_fold[\"sample\"]=0    \n",
    "        for x,y in zip(encode1,encode2):\n",
    "            r1,r2=encode2score[x],encode2score[y]\n",
    "            index1,index2=score2index[r1],score2index[r2]\n",
    "            if index1>index2:\n",
    "                index1,index2=index2,index1\n",
    "            comments_rank_fold.loc[index1:index2,\"sample\"]+=1\n",
    "        median_sample_num=comments_rank_fold[\"sample\"].median()\n",
    "        sample_comments=comments_rank_fold.query(\"sample>@median_sample_num\")\n",
    "        \n",
    "        train=pd.concat([train,sample_comments])\n",
    "    train=train.reset_index(drop=True)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=sample_error(5,origin_error_df)\n",
    "train.to_csv(\"./persedo_work/error_persedo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,axs=plt.subplots(3,3,figsize=(20,10))\n",
    "# sns.kdeplot(data=origin_error_df[\"less_value\"],ax=axs[0][0])\n",
    "# sns.kdeplot(data=origin_error_df[\"more_value\"],ax=axs[1][0])\n",
    "# sns.kdeplot(data=origin_error_df[\"more_value\"]-origin_error_df[\"less_value\"],ax=axs[2][0])\n",
    "\n",
    "# sns.kdeplot(data=origin_data_df[\"less_value\"],ax=axs[0][1])\n",
    "# sns.kdeplot(data=origin_data_df[\"more_value\"],ax=axs[1][1])\n",
    "# sns.kdeplot(data=origin_data_df[\"more_value\"]-origin_data_df[\"less_value\"],ax=axs[2][1])\n",
    "\n",
    "# sns.kdeplot(data=origin_true_df[\"less_value\"],ax=axs[0][2])\n",
    "# sns.kdeplot(data=origin_true_df[\"more_value\"],ax=axs[1][2])\n",
    "# sns.kdeplot(data=origin_true_df[\"more_value\"]-origin_true_df[\"less_value\"],ax=axs[2][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SpecificationError",
     "evalue": "nested renamer is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSpecificationError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a48d1fcc37ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m origin_data_df_agg=origin_data_df.groupby([\"label_min\",\"label_max\",\"label_min_text\",\"label_max_text\"\n\u001b[1;32m----> 2\u001b[1;33m                                         ]).agg({\"win_min\":\"sum\",\"win_max\":\"sum\",\"label_min_value\":\"mean\",\"label_max_value\":\"mean\",\"pair_True\":\"mean\"}).reset_index()\n\u001b[0m\u001b[0;32m      3\u001b[0m origin_data_df_agg[\"value_gap\"]=origin_data_df_agg.apply(lambda row:row[\"label_max_value\"]-row[\"label_min_value\"] \n\u001b[0;32m      4\u001b[0m                                                          if row[\"win_max\"]>row[\"win_min\"] else row[\"label_min_value\"]-row[\"label_max_value\"],axis=1)\n\u001b[0;32m      5\u001b[0m origin_data_df_agg[\"win_gap\"]=origin_data_df_agg.apply(lambda row:(row[\"win_max\"]-row[\"win_min\"])/(row[\"win_max\"]+row[\"win_min\"])\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    938\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_maybe_mangle_lambdas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_aggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhow\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_aggregate\u001b[1;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[0;32m    364\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m                 ) != len(keys):\n\u001b[1;32m--> 366\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mSpecificationError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nested renamer is not supported\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSpecificationError\u001b[0m: nested renamer is not supported"
     ]
    }
   ],
   "source": [
    "origin_data_df_agg=origin_data_df.groupby([\"label_min\",\"label_max\",\"label_min_text\",\"label_max_text\"\n",
    "                                        ]).agg({\"win_min\":\"sum\",\"win_max\":\"sum\",\"label_min_value\":\"mean\",\"label_max_value\":\"mean\",\"pair_True\":\"mean\"}).reset_index()\n",
    "origin_data_df_agg[\"value_gap\"]=origin_data_df_agg.apply(lambda row:row[\"label_max_value\"]-row[\"label_min_value\"] \n",
    "                                                         if row[\"win_max\"]>row[\"win_min\"] else row[\"label_min_value\"]-row[\"label_max_value\"],axis=1)\n",
    "origin_data_df_agg[\"win_gap\"]=origin_data_df_agg.apply(lambda row:(row[\"win_max\"]-row[\"win_min\"])/(row[\"win_max\"]+row[\"win_min\"])\n",
    "                                                         if row[\"win_max\"]>row[\"win_min\"] else (row[\"win_min\"]-row[\"win_max\"])/(row[\"win_max\"]+row[\"win_min\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8396319746735259"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2291 条数据错误 7817条正确\n",
    "origin_data_error=origin_data_df_agg[origin_data_df_agg[\"pair_True\"]<0.5]\n",
    "origin_data_true=origin_data_df_agg[origin_data_df_agg[\"pair_True\"]>0.5]\n",
    "len(origin_data_true)/len(origin_data_df_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origin_data_error.to_csv(os.path.join(input_dir,f\"aug_1\\\\origin_data_error.csv\"))\n",
    "# origin_data_true.to_csv(os.path.join(input_dir,f\"aug_1\\\\origin_data_true.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3905"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(origin_data_true[\"win_gap\"].value_counts())[1/3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#对正确的部分 划分为正1 正2 根据赢多少的比例来\n",
    "win_gap_num=dict(origin_data_true[\"win_gap\"].value_counts())\n",
    "true_point=win_gap_num[1/3]/(win_gap_num[1/3]+win_gap_num[1])\n",
    "true_val_gap=list(origin_data_true[\"value_gap\"].sort_values())\n",
    "true_point_value=true_val_gap[int(true_point*len(true_val_gap))]\n",
    "origin_data_true[\"degree\"]=origin_data_true.apply(lambda row:2 if row[\"value_gap\"]>true_point_value else 1,axis=1)\n",
    "\n",
    "origin_true_degree2=origin_data_true[origin_data_true[\"degree\"]==2]\n",
    "origin_true_degree1=origin_data_true[origin_data_true[\"degree\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#对错误的部分 也划分为 负1 和负2\n",
    "false_gap_num=dict(origin_data_error[\"win_gap\"].value_counts())\n",
    "false_point=false_gap_num[1/3]/(false_gap_num[1/3]+false_gap_num[1])\n",
    "false_val_gap=list(origin_data_error[\"value_gap\"].sort_values())\n",
    "false_point_value=false_val_gap[int(false_point*len(false_val_gap))]\n",
    "origin_data_error[\"degree\"]=origin_data_error.apply(lambda row:1 if row[\"value_gap\"]>false_point_value else 2,axis=1)\n",
    "\n",
    "origin_false_degree2=origin_data_error[origin_data_error[\"degree\"]==2]\n",
    "origin_false_degree1=origin_data_error[origin_data_error[\"degree\"]==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 绘图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
