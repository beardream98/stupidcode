{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de5e2806",
   "metadata": {
    "papermill": {
     "duration": 0.03161,
     "end_time": "2021-12-18T06:53:21.496384",
     "exception": false,
     "start_time": "2021-12-18T06:53:21.464774",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c1bb444f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-18T06:53:21.558452Z",
     "iopub.status.busy": "2021-12-18T06:53:21.557523Z",
     "iopub.status.idle": "2021-12-18T06:53:23.802326Z",
     "shell.execute_reply": "2021-12-18T06:53:23.801464Z",
     "shell.execute_reply.started": "2021-12-18T03:14:08.926859Z"
    },
    "papermill": {
     "duration": 2.277333,
     "end_time": "2021-12-18T06:53:23.802539",
     "exception": false,
     "start_time": "2021-12-18T06:53:21.525206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import scipy.optimize as optimize\n",
    "import lightgbm as lgb\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "135e7fe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-18T06:53:23.853527Z",
     "iopub.status.busy": "2021-12-18T06:53:23.852450Z",
     "iopub.status.idle": "2021-12-18T06:53:23.856656Z",
     "shell.execute_reply": "2021-12-18T06:53:23.857195Z",
     "shell.execute_reply.started": "2021-12-18T03:14:11.329494Z"
    },
    "papermill": {
     "duration": 0.030915,
     "end_time": "2021-12-18T06:53:23.857396",
     "exception": false,
     "start_time": "2021-12-18T06:53:23.826481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#1 means kf 2 means sample\n",
    "Fold_type=2\n",
    "is_test=False\n",
    "translate_aug=False\n",
    "#记得改kdict 里面的tranlate的值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f6966147",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-18T06:53:23.906916Z",
     "iopub.status.busy": "2021-12-18T06:53:23.905895Z",
     "iopub.status.idle": "2021-12-18T06:53:23.909694Z",
     "shell.execute_reply": "2021-12-18T06:53:23.910210Z",
     "shell.execute_reply.started": "2021-12-18T03:14:11.336152Z"
    },
    "papermill": {
     "duration": 0.030203,
     "end_time": "2021-12-18T06:53:23.910403",
     "exception": false,
     "start_time": "2021-12-18T06:53:23.880200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fold_num_k=5\n",
    "fold_num_s=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "5ae51b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n如果需要增加数据集 检查 data_names, 是否clean 检查clean data，create fold 处，name2dict处\\n'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "如果需要增加数据集 检查 data_names, 是否clean 检查clean data，create fold 处，name2dict处\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6ee0fa49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-18T06:53:23.960384Z",
     "iopub.status.busy": "2021-12-18T06:53:23.959334Z",
     "iopub.status.idle": "2021-12-18T06:53:23.963665Z",
     "shell.execute_reply": "2021-12-18T06:53:23.964266Z",
     "shell.execute_reply.started": "2021-12-18T06:51:41.303964Z"
    },
    "papermill": {
     "duration": 0.031328,
     "end_time": "2021-12-18T06:53:23.964459",
     "exception": false,
     "start_time": "2021-12-18T06:53:23.933131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_names=[\"jc_\",\"juc_\",\"rud_\",\"jcc_\"]\n",
    "# model_choice=[\"ridge\",\"gbm\"]\n",
    "# factor=[0.5,0.5]\n",
    "\n",
    "data_names=[\"jc_\",\"rud_\",\"jcc1_\",\"jcc2_\",\"jc_fr_\",\"jc_de_\",\"jc_es_\"]\n",
    "# data_names=[\"jc_fr_\",\"jc_de_\",\"jc_es_\"]\n",
    "\n",
    "translate_data=[\"jc_s_\"]\n",
    "translate_language=[\"fr_text\",\"de_text\",\"es_text\"]\n",
    "clean_data={\"jcc1_s_\":1,\"jcc2_s_\":2}\n",
    "model_choice=[\"ridge\"]\n",
    "factor=[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "41c56c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_path=r\"C:\\Users\\Lenovo\\Desktop\\stupidcode\\data\\jigsaw\"\n",
    "# model_all=[\"ridge\",\"gbm\"]\n",
    "# for model_name in model_all:\n",
    "#     if model_name not in os.listdir():\n",
    "#         os.makedirs(f\"{model_name}\")\n",
    "out_path=r\"C:\\Users\\Lenovo\\Desktop\\stupidcode\\data\\jigsaw\\save_model\\temp_work_save\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf7203",
   "metadata": {
    "papermill": {
     "duration": 0.022384,
     "end_time": "2021-12-18T06:53:24.010931",
     "exception": false,
     "start_time": "2021-12-18T06:53:23.988547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4c0e4ab7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-18T06:53:24.062286Z",
     "iopub.status.busy": "2021-12-18T06:53:24.061249Z",
     "iopub.status.idle": "2021-12-18T06:53:24.066219Z",
     "shell.execute_reply": "2021-12-18T06:53:24.066799Z",
     "shell.execute_reply.started": "2021-12-18T03:14:11.359204Z"
    },
    "papermill": {
     "duration": 0.031809,
     "end_time": "2021-12-18T06:53:24.066970",
     "exception": false,
     "start_time": "2021-12-18T06:53:24.035161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#第一届 jigsaw比赛 数据（challenge） Toxic Comment Classification Challenge\n",
    "\n",
    "jc_path=os.path.join(system_path,\"jigsaw-toxic-comment-classification-challenge\")\n",
    "jc_trans_path=os.path.join(system_path,\"jigsaw-toxic-comment-classification-challenge\")\n",
    "#ruddit 数据\n",
    "run_path=os.path.join(system_path,\"ruddit-jigsaw-dataset/Dataset\")\n",
    "#第二届 jigsaw比赛 对少数人群不歧视\n",
    "juc_path=os.path.join(system_path,\"jigsaw-unintended-bias-in-toxicity-classification\")\n",
    "\n",
    "#本次比赛数据 作为val\n",
    "jts_path=os.path.join(system_path,\"jigsaw-toxic-severity-rating\")\n",
    "\n",
    "# #数据抽样存储路径\n",
    "gbm_save_path=os.path.join(out_path,\"gbm\")\n",
    "ridge_save_path=os.path.join(out_path,\"ridge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b27fbcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#验证集和测试集\n",
    "df_val = pd.read_csv(os.path.join(jts_path,\"validation_data.csv\"))\n",
    "\n",
    "df_test = pd.read_csv(os.path.join(jts_path,\"comments_to_score.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f29b39c",
   "metadata": {
    "papermill": {
     "duration": 0.022185,
     "end_time": "2021-12-18T06:53:24.166454",
     "exception": false,
     "start_time": "2021-12-18T06:53:24.144269",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e9e6d142",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-18T06:53:24.974156Z",
     "iopub.status.busy": "2021-12-18T06:53:24.973083Z",
     "iopub.status.idle": "2021-12-18T06:53:28.861151Z",
     "shell.execute_reply": "2021-12-18T06:53:28.861703Z",
     "shell.execute_reply.started": "2021-12-18T03:14:12.088563Z"
    },
    "papermill": {
     "duration": 3.91504,
     "end_time": "2021-12-18T06:53:28.861921",
     "exception": false,
     "start_time": "2021-12-18T06:53:24.946881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments with toxic behaviour:16225\n"
     ]
    }
   ],
   "source": [
    "#第一届比赛数据 以0/1为分值 \n",
    "features = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "\n",
    "jc_train_df = pd.read_csv(os.path.join(jc_path,\"train.csv\"))\n",
    "# jc_test_df = pd.read_csv(os.path.join(jc_path,\"test.csv\"))\n",
    "# temp_df = pd.read_csv(os.path.join(jc_path,\"test_labels.csv\"))\n",
    "\n",
    "# jc_test_df = jc_test_df.merge ( temp_df, on =\"id\")\n",
    "# #drop test data not used for scoring\n",
    "# jc_test_df = jc_test_df.query (\"toxic != -1\")\n",
    "# jc_df = jc_train_df.append ( jc_test_df ) \n",
    "\n",
    "# print(f\"Train+Test:{jc_df.shape[0]}\")\n",
    "\n",
    "jc_df=jc_train_df\n",
    "\n",
    "# 将代表有毒行为的筛选出来\n",
    "jc_df[\"toxic_subtype_sum\"]=jc_df[features].sum(axis=1)\n",
    "jc_df[\"toxic_behaviour\"]=jc_df[\"toxic_subtype_sum\"].map(lambda x: x > 0)\n",
    "\n",
    "tot_toxic_behaviour = jc_df[\"toxic_behaviour\"].sum()\n",
    "print(f'comments with toxic behaviour:{tot_toxic_behaviour}')\n",
    "jc_df=jc_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "05b0dbb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 9)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第一届比赛 数据预处理\n",
    "# toxic = 1.0\n",
    "# severe_toxic = 2.0\n",
    "# obscene = 1.0\n",
    "# threat = 1.0\n",
    "# insult = 1.0\n",
    "# identity_hate = 2.0\n",
    "cat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n",
    "            'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n",
    "toxic_labels=[k for k in cat_mtpl.keys()]\n",
    "def create_train (df):\n",
    "    \n",
    "    for category in cat_mtpl:\n",
    "        df[category] = df[category] * cat_mtpl[category]\n",
    "    df['y'] = df.loc[:, toxic_labels].sum(axis=1)\n",
    "    \n",
    "    \n",
    "    df = df[[\"id\",'comment_text', 'y', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].rename(columns={'comment_text': 'text'})\n",
    "    return df\n",
    "        \n",
    "jc_df = create_train (jc_df)\n",
    "jc_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "89ee732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jc 翻译数据处理\n",
    "jc_trans=pd.read_csv(os.path.join(jc_trans_path,\"jc_trans.csv\"))\n",
    "jc_trans.set_index([\"Unnamed: 0\"], inplace=True)\n",
    "def translation_df(jc_trans,language_text):\n",
    "    df=pd.merge(jc_df,jc_trans[[\"id\",language_text]],on=\"id\",how=\"left\")\n",
    "    df[\"text\"]=df.apply(lambda row:row[\"text\"] if pd.isna(row[language_text]) else row[language_text],axis=1)\n",
    "    df.drop([language_text],axis=1,inplace=True)\n",
    "    return df\n",
    "jc_fr_df=translation_df(jc_trans,\"fr_text\")\n",
    "jc_es_df=translation_df(jc_trans,\"es_text\")\n",
    "jc_de_df=translation_df(jc_trans,\"de_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6f6ff9f1",
   "metadata": {
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2021-12-18T06:53:29.041822Z",
     "iopub.status.busy": "2021-12-18T06:53:29.041024Z",
     "iopub.status.idle": "2021-12-18T06:54:10.203186Z",
     "shell.execute_reply": "2021-12-18T06:54:10.202608Z",
     "shell.execute_reply.started": "2021-12-18T03:14:16.251944Z"
    },
    "papermill": {
     "duration": 41.195441,
     "end_time": "2021-12-18T06:54:10.203374",
     "exception": false,
     "start_time": "2021-12-18T06:53:29.007933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juc_df:(601044, 46)\n"
     ]
    }
   ],
   "source": [
    "# 第二届比赛 以打分形式0~1\n",
    "features = [\"toxicity\",\"severe_toxicity\",\"obscene\",\"insult\",\"identity_attack\", \"sexual_explicit\"]\n",
    "cols = ['id', 'comment_text', 'toxicity', 'severe_toxicity', 'obscene', 'threat','insult', 'identity_attack', 'sexual_explicit', 'toxicity_annotator_count']\n",
    "\n",
    "juc_df = pd.read_csv(os.path.join(juc_path,\"all_data.csv\"))\n",
    "\n",
    "juc_df = juc_df.query (\"toxicity_annotator_count > 5\")\n",
    "print(f\"juc_df:{juc_df.shape}\")\n",
    "juc_df['y'] = juc_df[[ 'severe_toxicity', 'obscene', 'sexual_explicit','identity_attack', 'insult', 'threat']].sum(axis=1)\n",
    "\n",
    "#毒性<0.5 按标注 否则算所有有毒行为的标注和\n",
    "juc_df['y'] = juc_df.apply(lambda row: row[\"toxicity\"] if row[\"toxicity\"] <= 1 else row[\"y\"] , axis=1)\n",
    "juc_df = juc_df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\n",
    "# min_len = (juc_df['y'] > 0.5).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "68612164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(247667, 2)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "juc_df.query(\"y<0.2\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "77cc151f",
   "metadata": {
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2021-12-18T06:54:10.258533Z",
     "iopub.status.busy": "2021-12-18T06:54:10.257751Z",
     "iopub.status.idle": "2021-12-18T06:54:10.333807Z",
     "shell.execute_reply": "2021-12-18T06:54:10.333123Z",
     "shell.execute_reply.started": "2021-12-18T03:14:57.931639Z"
    },
    "papermill": {
     "duration": 0.106101,
     "end_time": "2021-12-18T06:54:10.333955",
     "exception": false,
     "start_time": "2021-12-18T06:54:10.227854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rud_df:(5838, 5)\n"
     ]
    }
   ],
   "source": [
    "#ruddit 数据\n",
    "rud_df = pd.read_csv(os.path.join(run_path,\"ruddit_with_text.csv\"))\n",
    "\n",
    "print(f\"rud_df:{rud_df.shape}\")\n",
    "rud_df['y'] = rud_df['offensiveness_score'].map(lambda x: 0.0 if x <=0 else x)\n",
    "# rud_df['y'] = rud_df['offensiveness_score']\n",
    "\n",
    "rud_df = rud_df[['txt', 'y']].rename(columns={'txt': 'text'})\n",
    "dele_flag=\"[deleted]\"\n",
    "rud_df=rud_df.query(\"text!=@dele_flag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f5e6d899",
   "metadata": {
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2021-12-18T06:54:10.426760Z",
     "iopub.status.busy": "2021-12-18T06:54:10.425979Z",
     "iopub.status.idle": "2021-12-18T06:55:22.205549Z",
     "shell.execute_reply": "2021-12-18T06:55:22.206138Z",
     "shell.execute_reply.started": "2021-12-18T03:14:58.03388Z"
    },
    "papermill": {
     "duration": 71.848091,
     "end_time": "2021-12-18T06:55:22.206369",
     "exception": false,
     "start_time": "2021-12-18T06:54:10.358278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#clean data\n",
    "def clean1(text):\n",
    "\n",
    "    # Clean some punctutations\n",
    "    text=re.sub(r'\\n', r' \\n ',text)\n",
    "    text=re.sub(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3',text)\n",
    "    \n",
    "    # Replace repeating characters more than 3 times to length of 3\n",
    "    text=re.sub(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1',text)\n",
    "    \n",
    "    # Add space around repeating characters\n",
    "    text=re.sub(r'([*!?\\']+)',r' \\1 ',text)\n",
    "    \n",
    "    # patterns with repeating characters \n",
    "    text=re.sub(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1',text)\n",
    "    text=re.sub(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1',text)\n",
    "    text=re.sub(r'[ ]{2,}',' ',text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def clean2(text):\n",
    "    '''\n",
    "    Cleans text into a basic form for NLP. Operations include the following:-\n",
    "    1. Remove special charecters like &, #, etc\n",
    "    2. Removes extra spaces\n",
    "    3. Removes embedded URL links\n",
    "    4. Removes HTML tags\n",
    "    5. Removes emojis\n",
    "    \n",
    "    text - Text piece to be cleaned.\n",
    "    '''\n",
    "    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n",
    "    text = template.sub(r'', text)\n",
    "    \n",
    "    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n",
    "    only_text = soup.get_text()\n",
    "    text = only_text\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n",
    "    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n",
    "    text = text.strip() # remove spaces at the beginning and at the end of string\n",
    "\n",
    "    return text\n",
    "def clean3(text):\n",
    "    '''\n",
    "    1+2\n",
    "    '''\n",
    "    text=re.sub(r'\\n', r' \\n ',text)\n",
    "    text=re.sub(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3',text)\n",
    "    \n",
    "    # Replace repeating characters more than 3 times to length of 3\n",
    "    text=re.sub(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1',text)\n",
    "    \n",
    "    # Add space around repeating characters\n",
    "    text=re.sub(r'([*!?\\']+)',r' \\1 ',text)\n",
    "    \n",
    "    # patterns with repeating characters \n",
    "    text=re.sub(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1',text)\n",
    "    text=re.sub(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1',text)\n",
    "    text=re.sub(r'[ ]{2,}',' ',text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n",
    "    text = template.sub(r'', text)\n",
    "    \n",
    "    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n",
    "    only_text = soup.get_text()\n",
    "    text = only_text\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n",
    "    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n",
    "    text = text.strip() # remove spaces at the beginning and at the end of string\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f27d13c9",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# test_clean_df = pd.DataFrame({\"text\":\n",
    "#                               [\"heyy\\n\\nkkdsfj\",\n",
    "#                                \"hi   how/are/you ???\",\n",
    "#                                \"hey?????\",\n",
    "#                                \"hey????? 18.98.333.20 18.98.\",\n",
    "#                                \"noooo!!!!!!!!!   comeone !! \",\n",
    "#                               \"cooooooooool     brooooooooooo  coool brooo\",\n",
    "#                               \"naaaahhhhhhh\"]})\n",
    "# display(test_clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "1917e253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 159571/159571 [00:15<00:00, 9996.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 159571/159571 [01:14<00:00, 2137.19it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 159571/159571 [01:35<00:00, 1665.53it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "# clean 数据\n",
    "def clean_df(df,clean_type):\n",
    "    clean_df=df.copy()\n",
    "    \n",
    "    if clean_type==1:\n",
    "        \n",
    "        clean_df['text']=clean_df['text'].progress_apply(clean1)\n",
    "    elif clean_type==2:\n",
    "        \n",
    "        clean_df['text']=clean_df['text'].progress_apply(clean2)\n",
    "    elif clean_type==3:\n",
    "        clean_df['text']=clean_df['text'].progress_apply(clean3)\n",
    "    return clean_df\n",
    "jcc1_df=clean_df(jc_df,1)\n",
    "jcc2_df=clean_df(jc_df,2)\n",
    "jcc3_df=clean_df(jc_df,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c304e5",
   "metadata": {
    "papermill": {
     "duration": 0.023836,
     "end_time": "2021-12-18T06:55:22.254746",
     "exception": false,
     "start_time": "2021-12-18T06:55:22.230910",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 消重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b4e86687",
   "metadata": {
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2021-12-18T06:55:22.308806Z",
     "iopub.status.busy": "2021-12-18T06:55:22.308107Z",
     "iopub.status.idle": "2021-12-18T06:55:22.315245Z",
     "shell.execute_reply": "2021-12-18T06:55:22.315791Z",
     "shell.execute_reply.started": "2021-12-18T03:16:09.280445Z"
    },
    "papermill": {
     "duration": 0.036094,
     "end_time": "2021-12-18T06:55:22.315971",
     "exception": false,
     "start_time": "2021-12-18T06:55:22.279877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30108, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6489, 3)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 消除jc中重叠数据\n",
    "df_jc_val = pd.read_csv(os.path.join(jts_path,\"validation_data.csv\"))\n",
    "print(df_jc_val.shape)\n",
    "\n",
    "\n",
    "# Find cases already present in toxic data\n",
    "\n",
    "df_jc_val = pd.merge(df_jc_val, jc_df.loc[:,['text']], \n",
    "                  left_on = 'less_toxic', \n",
    "                  right_on = 'text', how='left')\n",
    "\n",
    "df_jc_val = pd.merge(df_jc_val, jc_df.loc[:,['text']], \n",
    "                  left_on = 'more_toxic', \n",
    "                  right_on = 'text', how='left')\n",
    "\n",
    "# Removing those cases\n",
    "# df_jc_val = df_jc_val[(~df_jc_val.text_x.isna()) | (~df_jc_val.text_y.isna())][['worker', 'less_toxic', 'more_toxic']]\n",
    "df_jc_val = df_jc_val[(df_jc_val.text_x.isna()) & (df_jc_val.text_y.isna())][['worker', 'less_toxic', 'more_toxic']]\n",
    "\n",
    "df_jc_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da9f08f",
   "metadata": {},
   "source": [
    "### aug_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d934cbd9",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if translate_aug==True:\n",
    "    jc_trans=pd.read_csv(os.path.join(jc_trans_path,\"jc_trans.csv\"))\n",
    "    jc_trans.set_index([\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "    jc_df=pd.merge(jc_df,jc_trans[[\"id\",\"fr_text\"]],on=\"id\",how=\"left\")\n",
    "    jc_df=pd.merge(jc_df,jc_trans[[\"id\",\"es_text\"]],on=\"id\",how=\"left\")\n",
    "    jc_df=pd.merge(jc_df,jc_trans[[\"id\",\"de_text\"]],on=\"id\",how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2719df77",
   "metadata": {},
   "source": [
    "## 分值显示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "97023c1d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def show_data(train_df):\n",
    "    printed = []\n",
    "    for i in sorted(train_df.y.unique()):\n",
    "        n = np.round(i, 2) \n",
    "        if n in printed:\n",
    "            continue\n",
    "        printed.append(n)\n",
    "        print(f\"{len(printed):<3}: {i:.5f}\\t{repr(np.random.choice(train_df[train_df.y==i]['text']))[:300]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05808bbc",
   "metadata": {},
   "source": [
    "# create fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "57d6c819",
   "metadata": {
    "code_folding": [
     0
    ],
    "execution": {
     "iopub.execute_input": "2021-12-18T06:55:22.367276Z",
     "iopub.status.busy": "2021-12-18T06:55:22.366637Z",
     "iopub.status.idle": "2021-12-18T06:55:22.387692Z",
     "shell.execute_reply": "2021-12-18T06:55:22.387117Z",
     "shell.execute_reply.started": "2021-12-18T03:16:09.291353Z"
    },
    "papermill": {
     "duration": 0.047762,
     "end_time": "2021-12-18T06:55:22.387846",
     "exception": false,
     "start_time": "2021-12-18T06:55:22.340084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_fold_kfold(df,FOLDS=5,select_num=0.5,frac_1=1.5,balance=False):\n",
    "    #select num challenge:1 unbias:0.5  \n",
    "\n",
    "    min_len = (df['y'] >= select_num).sum()\n",
    "    if balance==False:\n",
    "        #采样负样本\n",
    "        df_y0_undersample = df[df['y'] <select_num].sample(n=int(min_len*frac_1),random_state=201)\n",
    "        df = pd.concat([df[-(df['y'] <select_num)], df_y0_undersample])\n",
    "        df=df.reset_index(drop=True)\n",
    "    y=df[\"y\"].values\n",
    "    x=df[\"text\"]\n",
    "    stratified = np.around (y)\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=FOLDS,shuffle=True,random_state=123)\n",
    "    df_dict={}\n",
    "    for fold,(train_index,val_index) in enumerate(kf.split(x,stratified)):\n",
    "        df_train=df.iloc[train_index]\n",
    "        df_dict[fold]=df_train\n",
    "    \n",
    "    return df_dict\n",
    "\n",
    "def create_fold_sample(df,n_folds=3,frac_1=0.8,frac_1_factor=1.5,select_num=0,balance=False,translate=False):\n",
    "    df_dict={}\n",
    "    #正样本 大于等于select_num\n",
    "    select_list=-(df['y'] <select_num)\n",
    "    min_len=select_list.sum()\n",
    "    if translate==True:\n",
    "        min_len=4*min_len\n",
    "    for fld in range(n_folds):\n",
    "        if balance==False:\n",
    "#             df_y0_undersample=df[df.y<select_num].sample(n=int(min_len*frac_1*frac_1_factor), random_state = 10*(fld+1))\n",
    "            df_y0_undersample=df[df.y<select_num].sample(n=int(min_len*frac_1*frac_1_factor), random_state = 201)\n",
    "\n",
    "            tmp_df = pd.concat([df[select_list].sample(frac=frac_1, random_state = 10*(fld+1)),df_y0_undersample ])\n",
    "        else:\n",
    "            tmp_df = df.sample(frac=frac_1, random_state = 10*(fld+1))\n",
    "            \n",
    "        df_dict[fld]=tmp_df\n",
    "\n",
    "    return df_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b2d97788",
   "metadata": {
    "code_folding": [
     0
    ],
    "execution": {
     "iopub.execute_input": "2021-12-18T06:55:22.442197Z",
     "iopub.status.busy": "2021-12-18T06:55:22.441528Z",
     "iopub.status.idle": "2021-12-18T06:55:22.444122Z",
     "shell.execute_reply": "2021-12-18T06:55:22.444608Z",
     "shell.execute_reply.started": "2021-12-18T03:16:09.315307Z"
    },
    "papermill": {
     "duration": 0.032813,
     "end_time": "2021-12-18T06:55:22.444789",
     "exception": false,
     "start_time": "2021-12-18T06:55:22.411976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_test==True:\n",
    "    jc_df=jc_df[0:400]\n",
    "    juc_df=juc_df[0:400]\n",
    "    rud_df=rud_df[0:400]\n",
    "    jcc_df=jcc_df[0:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "bf57e88f",
   "metadata": {
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2021-12-18T06:55:22.508545Z",
     "iopub.status.busy": "2021-12-18T06:55:22.507748Z",
     "iopub.status.idle": "2021-12-18T06:55:23.559608Z",
     "shell.execute_reply": "2021-12-18T06:55:23.560193Z",
     "shell.execute_reply.started": "2021-12-18T03:16:09.328508Z"
    },
    "papermill": {
     "duration": 1.091517,
     "end_time": "2021-12-18T06:55:23.560396",
     "exception": false,
     "start_time": "2021-12-18T06:55:22.468879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_test==True:\n",
    "#     jc_df_kdict=create_fold_kfold(jc_df,FOLDS=fold_num_k,select_num=1,frac_1=1.5,balance=True)\n",
    "#     juc_df_kdict=create_fold_kfold(juc_df,FOLDS=fold_num_k,select_num=0.5,frac_1=1.5,balance=True)\n",
    "#     rud_df_kdict=create_fold_kfold(rud_df,FOLDS=fold_num_k,select_num=0.5,frac_1=1.5,balance=True)\n",
    "#     jcc_df_kdict=create_fold_kfold(jcc_df,FOLDS=fold_num_k,select_num=1,frac_1=1.5,balance=True)\n",
    "\n",
    "    jc_df_sdict=create_fold_sample(jc_df,n_folds=fold_num_s,frac_1=0.8,frac_1_factor=1.5,select_num=0.001,balance=True,translate=translate_aug)\n",
    "    juc_df_sdict=create_fold_sample(juc_df,n_folds=fold_num_s,frac_1=0.8,frac_1_factor=1.5,select_num=0.5,balance=True)\n",
    "    rud_df_sdict=create_fold_sample(rud_df,n_folds=fold_num_s,frac_1=0.8,frac_1_factor=1.5,select_num=0.5,balance=True)\n",
    "    jcc1_df_sdict=create_fold_sample(jcc1_df,n_folds=fold_num_s,frac_1=0.8,frac_1_factor=1.5,select_num=0.001,balance=True)\n",
    "    jcc2_df_sdict=create_fold_sample(jcc2_df,n_folds=fold_num_s,frac_1=0.8,frac_1_factor=1.5,select_num=0.001,balance=True)\n",
    "    \n",
    "if is_test==False:\n",
    "#     jc_df_kdict=create_fold_kfold(jc_df,FOLDS=fold_num_k,select_num=1,frac_1=1.5,balance=False)\n",
    "#     juc_df_kdict=create_fold_kfold(juc_df,FOLDS=fold_num_k,select_num=0.5,frac_1=1.5,balance=False)\n",
    "#     rud_df_kdict=create_fold_kfold(rud_df,FOLDS=fold_num_k,select_num=0.5,frac_1=1.5,balance=True)\n",
    "#     jcc_df_kdict=create_fold_kfold(jcc_df,FOLDS=fold_num_k,select_num=1,frac_1=1.5,balance=False)\n",
    "\n",
    "    jc_df_sdict=create_fold_sample(jc_df,n_folds=fold_num_s,frac_1=1,frac_1_factor=1.5,select_num=0.001,balance=False,translate=translate_aug)\n",
    "    juc_df_sdict=create_fold_sample(juc_df,n_folds=fold_num_s,frac_1=0.8,frac_1_factor=1,select_num=0.5,balance=False)\n",
    "    rud_df_sdict=create_fold_sample(rud_df,n_folds=fold_num_s,frac_1=1,frac_1_factor=1,select_num=0.5,balance=True)\n",
    "    jcc1_df_sdict=create_fold_sample(jcc1_df,n_folds=fold_num_s,frac_1=1,frac_1_factor=1.5,select_num=0.001,balance=False)\n",
    "    jcc2_df_sdict=create_fold_sample(jcc2_df,n_folds=fold_num_s,frac_1=1,frac_1_factor=1.5,select_num=0.001,balance=False)\n",
    "    jc_fr_df_sdict=create_fold_sample(jc_fr_df,n_folds=fold_num_s,frac_1=1,frac_1_factor=1.5,select_num=0.001,balance=False)\n",
    "    jc_de_df_sdict=create_fold_sample(jc_de_df,n_folds=fold_num_s,frac_1=1,frac_1_factor=1.5,select_num=0.001,balance=False)\n",
    "    jc_es_df_sdict=create_fold_sample(jc_es_df,n_folds=fold_num_s,frac_1=1,frac_1_factor=1.5,select_num=0.001,balance=False)\n",
    "    \n",
    "    \n",
    "#     jc_df_kdict[0].shape\n",
    "#     juc_df_kdict[0].shape\n",
    "#     rud_df_kdict[0].shape\n",
    "#     jcc_df_sdict[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6120e172",
   "metadata": {
    "papermill": {
     "duration": 0.024421,
     "end_time": "2021-12-18T06:55:23.657242",
     "exception": false,
     "start_time": "2021-12-18T06:55:23.632821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e89e0f8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-18T06:55:23.720942Z",
     "iopub.status.busy": "2021-12-18T06:55:23.713880Z",
     "iopub.status.idle": "2021-12-18T06:55:23.724281Z",
     "shell.execute_reply": "2021-12-18T06:55:23.723606Z",
     "shell.execute_reply.started": "2021-12-18T03:16:10.388403Z"
    },
    "papermill": {
     "duration": 0.043143,
     "end_time": "2021-12-18T06:55:23.724452",
     "exception": false,
     "start_time": "2021-12-18T06:55:23.681309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "def ridge_cv(df_dic,n_folds,model_pre=\"jc_k_ridge_\",df_val=df_val,clean_prm=0,translate=False):\n",
    "    val_preds_arr1 = np.zeros((df_val.shape[0], n_folds))\n",
    "    val_preds_arr2 = np.zeros((df_val.shape[0], n_folds))\n",
    "    test_preds_arr = np.zeros((df_test.shape[0], n_folds))\n",
    "    for fld in tqdm(range(n_folds)):\n",
    "        df = df_dic[fld]\n",
    "        vec = TfidfVectorizer(analyzer='char_wb', max_df=0.5, min_df=3, ngram_range=(3, 5) )\n",
    "        vec_pre=model_pre+\"vec_\"\n",
    "        if translate!=True:\n",
    "            text=df[\"text\"]\n",
    "            y=df[\"y\"]\n",
    "        else:\n",
    "            trans_df=df.dropna(axis=0,subset = [\"fr_text\"])\n",
    "            text=pd.concat([df[\"text\"],trans_df[\"fr_text\"],trans_df[\"es_text\"],trans_df[\"de_text\"]])\n",
    "            y=pd.concat([df[\"y\"],trans_df[\"y\"],trans_df[\"y\"],trans_df[\"y\"]])\n",
    "        X=vec.fit_transform(text)\n",
    "        joblib.dump(vec,os.path.join(ridge_save_path,f'{vec_pre}{fld}.pkl')) #保存模型 文件后缀为.pkl\n",
    "        model=Ridge(alpha=0.5)\n",
    "        model.fit(X,y)\n",
    "\n",
    "        if clean_prm==1:\n",
    "            X_less_toxic = df_val.apply(lambda row :clean1(row[\"less_toxic\"]),axis=1)\n",
    "            X_more_toxic = df_val.apply(lambda row :clean1(row[\"more_toxic\"]),axis=1)\n",
    "            X_test = df_test.apply(lambda row :clean1(row[\"text\"]),axis=1)\n",
    "\n",
    "        elif clean_prm==2:\n",
    "            X_less_toxic = df_val.apply(lambda row :clean2(row[\"less_toxic\"]),axis=1)\n",
    "            X_more_toxic = df_val.apply(lambda row :clean2(row[\"more_toxic\"]),axis=1)\n",
    "            X_test = df_test.apply(lambda row :clean2(row[\"text\"]),axis=1)\n",
    "        elif clean_prm==3:\n",
    "\n",
    "            X_less_toxic = df_val.apply(lambda row :clean3(row[\"less_toxic\"]),axis=1)\n",
    "            X_more_toxic = df_val.apply(lambda row :clean3(row[\"more_toxic\"]),axis=1)\n",
    "            X_test = df_test.apply(lambda row :clean3(row[\"text\"]),axis=1)\n",
    "\n",
    "        else:\n",
    "            X_less_toxic = df_val['less_toxic']\n",
    "            X_more_toxic = df_val['more_toxic']\n",
    "            X_test = df_test['text']\n",
    "            \n",
    "        X_less_toxic = vec.transform(X_less_toxic)\n",
    "        X_more_toxic = vec.transform(X_more_toxic)\n",
    "        X_test = vec.transform(X_test)\n",
    "        val_preds_arr1[:,fld] = model.predict(X_less_toxic)\n",
    "        val_preds_arr2[:,fld] = model.predict(X_more_toxic)\n",
    "\n",
    "        test_preds_arr[:,fld] = model.predict(X_test)\n",
    "            \n",
    "        joblib.dump(model,os.path.join(ridge_save_path,f'{model_pre}{fld}.pkl')) #保存模型 文件后缀为.pkl\n",
    "        del model,vec\n",
    "    p1=val_preds_arr1.mean(axis=1)\n",
    "    p2=val_preds_arr2.mean(axis=1)\n",
    "    pv=test_preds_arr.mean(axis=1)\n",
    "    print(f'Validation Accuracy is { np.round((p1 < p2).mean() * 100,2)}')        \n",
    "    return p1,p2,pv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb13f35",
   "metadata": {
    "papermill": {
     "duration": 0.024087,
     "end_time": "2021-12-18T06:55:23.773679",
     "exception": false,
     "start_time": "2021-12-18T06:55:23.749592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "03b4bc13",
   "metadata": {
    "code_folding": [
     0
    ],
    "execution": {
     "iopub.execute_input": "2021-12-18T06:55:23.825951Z",
     "iopub.status.busy": "2021-12-18T06:55:23.824980Z",
     "iopub.status.idle": "2021-12-18T06:55:23.843421Z",
     "shell.execute_reply": "2021-12-18T06:55:23.842808Z",
     "shell.execute_reply.started": "2021-12-18T03:16:10.40443Z"
    },
    "papermill": {
     "duration": 0.045757,
     "end_time": "2021-12-18T06:55:23.843567",
     "exception": false,
     "start_time": "2021-12-18T06:55:23.797810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#verbose =-1 忽略警告\n",
    "depth=1000\n",
    "params = {'task':'train',\n",
    "    \"device\" : \"cpu\",\n",
    "    'boosting_type':'gbdt',\n",
    "    \"max_depth\":7,\n",
    "    \"num_leaves\":80,\n",
    "    \"bagging_fraction\":0.8,\n",
    "          \"bagging_freq\":5,\n",
    "    'objective':'regression',\n",
    "    \"learning_rate\":0.05,\n",
    "    \"max_bin\":63,\n",
    "    \"random_state\":2021,\n",
    "    'verbose':-1, \n",
    "    \"train_metric\":False,\n",
    "    \"force_col_wise\":False}\n",
    "\n",
    "\n",
    "def lightgbm_cv(df_dic,n_folds,model_pre=\"jc_k_gbm_\",df_val=df_val,clean_prm=False,translate=False):\n",
    "    ####lightgbm_cv 在aug情况下有bug\n",
    "    val_preds_arr1 = np.zeros((df_val.shape[0], n_folds))\n",
    "    val_preds_arr2 = np.zeros((df_val.shape[0], n_folds))\n",
    "    test_preds_arr = np.zeros((df_test.shape[0], n_folds))\n",
    "\n",
    "    for fld in tqdm(range(n_folds)):\n",
    "        df = df_dic[fld]\n",
    "        vec = TfidfVectorizer(analyzer='char_wb', max_df=0.5, min_df=3, ngram_range=(3, 5) )\n",
    "        if translate!=True:\n",
    "            text=df[\"text\"]\n",
    "            y=df[\"y\"]\n",
    "        else:\n",
    "            trans_df=df.dropna(axis=0,subset = [\"fr_text\"])\n",
    "            text=pd.concat([df[\"text\"],trans_df[\"fr_text\"],trans_df[\"es_text\"],trans_df[\"de_text\"]])\n",
    "            y=pd.concat([df[\"y\"],trans_df[\"y\"],trans_df[\"y\"],trans_df[\"y\"]])\n",
    "\n",
    "        X=vec.fit_transform(text)\n",
    "        vec_pre=model_pre+\"vec_\"\n",
    "        joblib.dump(vec,os.path.join(gbm_save_path,f'{vec_pre}{fld}.pkl')) #保存模型 文件后缀为.pkl\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X,y,test_size = 0.1,random_state = 0)\n",
    "        lgb_train = lgb.Dataset(X_train,y_train)\n",
    "        lgb_eval = lgb.Dataset(X_val,y_val,reference=lgb_train)\n",
    "        \n",
    "        gbm = lgb.train(params,lgb_train,\n",
    "            num_boost_round=300,\n",
    "            valid_sets=lgb_eval,\n",
    "            early_stopping_rounds=50\n",
    "            ) \n",
    "        \n",
    "        if clean_prm==True:\n",
    "            X_less_toxic = vec.transform(clean(df_val,'less_toxic')['less_toxic'])\n",
    "            X_more_toxic = vec.transform(clean(df_val,'more_toxic')['more_toxic'])\n",
    "            X_test = vec.transform(clean(df_test,'text')['text'])\n",
    "        else:\n",
    "            X_less_toxic = vec.transform(df_val['less_toxic'])\n",
    "            X_more_toxic = vec.transform(df_val['more_toxic'])\n",
    "            X_test = vec.transform(df_test['text'])\n",
    "    \n",
    "        val_preds_arr1[:,fld] = gbm.predict(X_less_toxic,num_iteration=gbm.best_iteration)\n",
    "        val_preds_arr2[:,fld] = gbm.predict(X_more_toxic,num_iteration=gbm.best_iteration)\n",
    "\n",
    "        test_preds_arr[:,fld] =gbm.predict(X_test,num_iteration=gbm.best_iteration)\n",
    "        \n",
    "        gbm.save_model(os.path.join(gbm_save_path,f'{model_pre}{fld}.txt'))\n",
    "        # 模型加载\n",
    "#         gbm = lgb.Booster(model_file='model.txt')\n",
    "        # 模型预测\n",
    "#         y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "        del gbm,vec\n",
    "    p1=val_preds_arr1.mean(axis=1)\n",
    "    p2=val_preds_arr2.mean(axis=1)\n",
    "    pv=test_preds_arr.mean(axis=1)\n",
    "    print(f'Validation Accuracy is { np.round((p1 < p2).mean() * 100,2)}')        \n",
    "    return p1,p2,pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "12f73136",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-18T06:55:23.916991Z",
     "iopub.status.busy": "2021-12-18T06:55:23.916147Z",
     "iopub.status.idle": "2021-12-18T07:08:31.129684Z",
     "shell.execute_reply": "2021-12-18T07:08:31.127026Z",
     "shell.execute_reply.started": "2021-12-18T03:16:10.429311Z"
    },
    "papermill": {
     "duration": 787.261682,
     "end_time": "2021-12-18T07:08:31.129841",
     "exception": false,
     "start_time": "2021-12-18T06:55:23.868159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:58<00:00, 58.05s/it]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy is 67.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:32<00:00, 32.28s/it]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy is 65.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:05<00:00, 65.80s/it]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy is 67.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:22<00:00, 82.38s/it]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy is 67.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:57<00:00, 57.40s/it]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy is 66.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:54<00:00, 54.50s/it]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy is 65.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:55<00:00, 55.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy is 66.2\n",
      " Validation Accuracy is 67.6465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "p1=defaultdict()\n",
    "p2=defaultdict()\n",
    "pv=defaultdict()\n",
    "\n",
    "val_data=df_val\n",
    "\n",
    "func_dict={\"ridge\":ridge_cv,\"gbm\":lightgbm_cv}\n",
    "# func_dict.get(x)\n",
    "\n",
    "if Fold_type==1:\n",
    "#     pre_names=[\"jc_k_\",\"juc_k_\",\"rud_k_\",\"jcc_k_\"]\n",
    "    pre_names=[ data_name+\"k_\" for data_name in data_names]\n",
    "    name2dict={\"jc_k_\":jc_df_kdict,\"juc_k_\":juc_df_kdict,\"rud_k_\":rud_df_kdict,\n",
    "               \"jcc1_k_\":jcc1_df_kdict,\"jcc2_k_\":jcc2_df_kdict}\n",
    "    fold_num=fold_num_k\n",
    "elif Fold_type==2:\n",
    "    pre_names=[ data_name+\"s_\" for data_name in data_names]\n",
    "    name2dict={\"jc_s_\":jc_df_sdict,\"juc_s_\":juc_df_sdict,\"rud_s_\":rud_df_sdict,\n",
    "               \"jcc1_s_\":jcc1_df_sdict,\"jcc2_s_\":jcc2_df_sdict,\n",
    "               \"jc_fr_s_\":jc_fr_df_sdict,\"jc_de_s_\":jc_de_df_sdict,\"jc_es_s_\":jc_es_df_sdict}\n",
    "    fold_num=fold_num_s\n",
    "\n",
    "p1_ensenmble = np.zeros((val_data.shape[0]))\n",
    "p2_ensenmble = np.zeros((val_data.shape[0]))\n",
    "score=np.zeros((df_test.shape[0]))\n",
    "\n",
    "for pre_name in pre_names:\n",
    "    ###model_pre_ridge:jc_s_ridge_ pre_name:jc_s_ model:jc_s_ridge_{fold} vec:jc_s_ridge_vec_{fold}\n",
    "    #pre_name jc_s_ model_name jc_s_ridge_\n",
    "    clean_prm=False\n",
    "    translate=False\n",
    "    \n",
    "    p1[pre_name],p2[pre_name]=np.zeros((val_data.shape[0])),np.zeros((val_data.shape[0]))\n",
    "    pv[pre_name]=np.zeros((df_test.shape[0]))\n",
    "    if pre_name in clean_data.keys():\n",
    "        clean_prm=clean_data[pre_name]\n",
    "    if any([ trans_name in pre_name for trans_name in translate_data]) and translate_aug==True:\n",
    "        translate=True\n",
    "\n",
    "    for index,model_name in enumerate(model_choice):\n",
    "        cv_func=func_dict.get(model_name)\n",
    "        model_pre=pre_name+model_name+\"_\"\n",
    "        p1[model_pre],p2[model_pre],pv[model_pre]=cv_func(name2dict[pre_name],n_folds=fold_num,df_val=val_data,\n",
    "                                                        model_pre=model_pre,clean_prm=clean_prm,translate=translate)\n",
    "\n",
    "        p1[pre_name]= p1[pre_name]+ p1[model_pre]*factor[index]\n",
    "        p2[pre_name]= p2[pre_name]+ p2[model_pre]*factor[index]\n",
    "        pv[pre_name]= pv[pre_name]+ pv[model_pre]*factor[index]\n",
    "\n",
    "    kmax=max(p1[pre_name].max(),p2[pre_name].max())\n",
    "    p1_ensenmble=p1_ensenmble+p1[pre_name]/kmax\n",
    "    p2_ensenmble=p2_ensenmble+p2[pre_name]/kmax\n",
    "    score=score+pv[pre_name]/kmax\n",
    "\n",
    "print(f' Validation Accuracy is { np.round((p1_ensenmble < p2_ensenmble).mean() * 100,4)}') \n",
    "\n",
    "    \n",
    "df_test['score'] = rankdata(score, method='ordinal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "1be3e704",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-18T07:08:31.195680Z",
     "iopub.status.busy": "2021-12-18T07:08:31.194953Z",
     "iopub.status.idle": "2021-12-18T07:08:31.221473Z",
     "shell.execute_reply": "2021-12-18T07:08:31.222058Z",
     "shell.execute_reply.started": "2021-12-18T03:20:30.345465Z"
    },
    "papermill": {
     "duration": 0.060847,
     "end_time": "2021-12-18T07:08:31.222247",
     "exception": false,
     "start_time": "2021-12-18T07:08:31.161400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60706f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 920.432941,
   "end_time": "2021-12-18T07:08:32.428720",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-18T06:53:11.995779",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
