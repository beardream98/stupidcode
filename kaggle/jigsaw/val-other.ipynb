{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport random\nimport time\nimport string\nfrom collections import defaultdict, deque\n\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup, logging\nfrom transformers import BertConfig, BertTokenizer, BertModel\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, TensorDataset, SequentialSampler, RandomSampler, DataLoader\nfrom torch.cuda.amp import autocast as autocast\nfrom torch.cuda.amp import  GradScaler\n\nfrom torch.optim import lr_scheduler\nfrom tqdm.notebook import tqdm\n\nimport gc; gc.enable()\nfrom IPython.display import clear_output\nfrom torch.optim.swa_utils import AveragedModel, SWALR\nfrom sklearn.model_selection import StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nlogging.set_verbosity_error()","metadata":{"papermill":{"duration":9.26997,"end_time":"2021-12-23T03:28:36.18541","exception":false,"start_time":"2021-12-23T03:28:26.91544","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-12T13:37:16.082853Z","iopub.execute_input":"2022-01-12T13:37:16.083736Z","iopub.status.idle":"2022-01-12T13:37:25.516072Z","shell.execute_reply.started":"2022-01-12T13:37:16.083623Z","shell.execute_reply":"2022-01-12T13:37:25.515067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For colored terminal text\nfrom colorama import Fore, Back, Style\nb_ = Fore.BLUE\ny_ = Fore.YELLOW\nsr_ = Style.RESET_ALL\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#产生一个哈希值\n# def id_generator(size=12, chars=string.ascii_lowercase + string.digits):\n#     return ''.join(random.SystemRandom().choice(chars) for _ in range(size))\n\n# HASH_NAME = id_generator(size=12)\n","metadata":{"papermill":{"duration":0.043418,"end_time":"2021-12-23T03:28:50.176898","exception":false,"start_time":"2021-12-23T03:28:50.13348","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-12T13:37:25.518066Z","iopub.execute_input":"2022-01-12T13:37:25.518424Z","iopub.status.idle":"2022-01-12T13:37:25.527107Z","shell.execute_reply.started":"2022-01-12T13:37:25.518367Z","shell.execute_reply":"2022-01-12T13:37:25.524612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# config","metadata":{"papermill":{"duration":0.032281,"end_time":"2021-12-23T03:28:50.24189","exception":false,"start_time":"2021-12-23T03:28:50.209609","status":"completed"},"tags":[]}},{"cell_type":"code","source":"DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nCONFIG={\n    \"TRAIN_BATCH_SIZE\":32,\n    \"MAX_LENGTH\":128,\n    \"DEV_BATCH_SIZE\": 64,\n    \"LR\":1e-4,\n    \"EPS\":1e-8,\n    \"weight_decay\":1e-6,\n    \n    \"scheduler\": 'CosineAnnealingLR',\n    \"min_lr\": 1e-6,\n    \"T_max\": 500,\n    \"T_0\":500,\n    \"margin\":0.5,\n    \"fold_num\":5,\n    \"seed\":2021,\n    \"num_class\":1,\n    \n    \"EPOCHS\":2,\n    \"evaluate_step\":None,\n    \"swa_start\":2,\n    \"model_init_lr\":0.9e-4,\n    \"multiplier\":0.9,\n    \"classifier_lr\":1e-4 ,\n    \"swa_lr\": 1e-5\n}\n\n\ninput_dir=\"../input/jigsaw-toxic-severity-rating\"\n","metadata":{"papermill":{"duration":0.102339,"end_time":"2021-12-23T03:28:50.376474","exception":false,"start_time":"2021-12-23T03:28:50.274135","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-12T13:37:25.528474Z","iopub.execute_input":"2022-01-12T13:37:25.528759Z","iopub.status.idle":"2022-01-12T13:37:25.59945Z","shell.execute_reply.started":"2022-01-12T13:37:25.528716Z","shell.execute_reply":"2022-01-12T13:37:25.598071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"更换模型","metadata":{}},{"cell_type":"code","source":"hidden_size=\"hidden_size\"\nnum_hidden_layers=\"num_hidden_layers\"\n#for xlnet\n# hidden_size=\"d_model\"\n# num_hidden_layers=\"n_layer\"\nMODEL_DIR=\"../input/roberta-transformers-pytorch/roberta-base\"\n# MODEL_DIR=\"../input/pretrained-albert-pytorch/albert-base-v1\"\n# MODEL_DIR=\"../input/transformers/xlnet-base-cased\"","metadata":{"execution":{"iopub.status.busy":"2022-01-12T13:37:25.604284Z","iopub.execute_input":"2022-01-12T13:37:25.605197Z","iopub.status.idle":"2022-01-12T13:37:25.611805Z","shell.execute_reply.started":"2022-01-12T13:37:25.605158Z","shell.execute_reply":"2022-01-12T13:37:25.610632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##检查事项\n* 提交之前 注意 run_db 是否打开 是否创建了正确的hash值\n* test是否关闭\n* 如果 换模型 model_dir 是否正确 model struct是否正确\n* gpu 是否需要打开","metadata":{}},{"cell_type":"code","source":"DATASET_TEST=False\nrun_db=True\nmodel_struct=\"OriginModel\"\n\nHASH_NAME=\"复现starter \"\n\nswa_use=False\ndata_aug=False\ntranslate_aug=False\nFP16=False\n\n#OriginModel MeanPoolingModel LastLayerCLSModel MaxPoolingModel\n#SecondToLastLayerCLSModel ConcatenateLastFourModel WeightedLayerPoolingModel WeightedLayerPoolingModel\n#AttentionPoolingModel\ntranslate_text=[\"text_fr\",\"text_de\",\"text_es\"]\n\nCONFIG['group'] = f'{HASH_NAME}-Baseline'\n","metadata":{"papermill":{"duration":0.042294,"end_time":"2021-12-23T03:28:50.9876","exception":false,"start_time":"2021-12-23T03:28:50.945306","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-12T13:37:25.613377Z","iopub.execute_input":"2022-01-12T13:37:25.613921Z","iopub.status.idle":"2022-01-12T13:37:25.626711Z","shell.execute_reply.started":"2022-01-12T13:37:25.613876Z","shell.execute_reply":"2022-01-12T13:37:25.625089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# W&B","metadata":{"papermill":{"duration":0.033229,"end_time":"2021-12-23T03:28:51.11638","exception":false,"start_time":"2021-12-23T03:28:51.083151","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=api_key)\n    anony = None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')","metadata":{"papermill":{"duration":2.957967,"end_time":"2021-12-23T03:28:54.108287","exception":false,"start_time":"2021-12-23T03:28:51.15032","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-12T13:37:25.631706Z","iopub.execute_input":"2022-01-12T13:37:25.632738Z","iopub.status.idle":"2022-01-12T13:37:28.386698Z","shell.execute_reply.started":"2022-01-12T13:37:25.632689Z","shell.execute_reply":"2022-01-12T13:37:28.385635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 数据处理","metadata":{"papermill":{"duration":0.032929,"end_time":"2021-12-23T03:28:54.175247","exception":false,"start_time":"2021-12-23T03:28:54.142318","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data_df=pd.read_csv(os.path.join(input_dir,\"validation_data.csv\"))","metadata":{"execution":{"iopub.status.busy":"2022-01-12T13:37:28.390871Z","iopub.execute_input":"2022-01-12T13:37:28.39138Z","iopub.status.idle":"2022-01-12T13:37:28.956229Z","shell.execute_reply.started":"2022-01-12T13:37:28.391342Z","shell.execute_reply":"2022-01-12T13:37:28.955303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"数据增强","metadata":{"papermill":{"duration":0.033961,"end_time":"2021-12-23T03:28:54.242352","exception":false,"start_time":"2021-12-23T03:28:54.208391","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ndef generate_comments(data):\n    more_toxic_text=data[\"more_toxic\"].values\n    less_toxic_text=data[\"less_toxic\"].values    \n    comments=np.concatenate((more_toxic_text,less_toxic_text))\n    comments=np.unique(comments)\n    comments=pd.DataFrame({\"text\":comments})\n    text_encoder=LabelEncoder()\n    text_encoder.fit(comments)\n    comments[\"encode_text\"]=text_encoder.transform(comments[\"text\"])\n    comments[\"toxic_value\"]=0\n    comments[\"access_time\"]=0\n    data[\"encode_less\"]=text_encoder.transform(data[\"less_toxic\"])\n    data[\"encode_more\"]=text_encoder.transform(data[\"more_toxic\"])\n    \n    return data,comments\n","metadata":{"papermill":{"duration":0.045101,"end_time":"2021-12-23T03:28:54.32118","exception":false,"start_time":"2021-12-23T03:28:54.276079","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-12T13:37:28.95898Z","iopub.execute_input":"2022-01-12T13:37:28.95958Z","iopub.status.idle":"2022-01-12T13:37:28.968916Z","shell.execute_reply.started":"2022-01-12T13:37:28.959534Z","shell.execute_reply":"2022-01-12T13:37:28.967812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bsearch(start,more2less_dict):\n    queue = deque([start])\n    visit_list=[]\n    while len(queue)!=0:\n        visit_id=queue.popleft()\n        if visit_id in visit_list:\n            continue\n        visit_list.append(visit_id)\n        queue+=deque(more2less_dict[visit_id])\n    visit_list.remove(start)\n    return [ x for x in visit_list if x not in more2less_dict[start] ]\n\ndef search_lessText(more2less_dict):\n    aug_dict= defaultdict(list)\n    for start in list(more2less_dict.keys()):\n        \n        aug_list=bsearch(start,more2less_dict)\n        aug_dict[start]=aug_list\n    return aug_dict\n\ndef data_aug1(data_df,comments):\n    data_df[\"label_min\"]=data_df.apply(lambda row:row[\"encode_less\"] \n                                   if row[\"encode_less\"]<row[\"encode_more\"] else row[\"encode_more\"],axis=1)\n    data_df[\"label_max\"]=data_df.apply(lambda row:row[\"encode_more\"] \n                                       if row[\"encode_less\"]<row[\"encode_more\"] else row[\"encode_less\"],axis=1)\n\n    data_df[\"win_min\"]=data_df.apply(lambda row:1 if row[\"encode_more\"]<row[\"encode_less\"] else 0 ,axis=1)\n    data_df[\"win_max\"]=data_df.apply(lambda row:0 if row[\"encode_more\"]<row[\"encode_less\"] else 1 ,axis=1)\n\n    data_df_agg=data_df.groupby([\"label_min\",\"label_max\"]).agg({\"win_min\":\"sum\",\"win_max\":\"sum\"}).reset_index()\n    data_df_agg[\"encode_less\"]=data_df_agg.apply(lambda row:row[\"label_min\"] \n                                                 if row[\"win_min\"]<row[\"win_max\"] else row[\"label_max\"],axis=1)\n    data_df_agg[\"encode_more\"]=data_df_agg.apply(lambda row:row[\"label_min\"] \n                                                 if row[\"win_min\"]>row[\"win_max\"] else row[\"label_max\"],axis=1)\n    \n    more2less_dict= defaultdict(list)\n    data_df_agg.apply(lambda row:more2less_dict[row[\"encode_more\"]].append(row[\"encode_less\"]),axis=1)\n    \n    aug_dict=search_lessText(more2less_dict)\n    aug_dict={key:value for key,value in aug_dict.items() if len(value)!=0}\n    aug_df=pd.DataFrame(columns=(tuple(data_df.columns)))\n    \n    id2text_dict=comments.to_dict()[\"text\"]\n    \n    for key,value in aug_dict.items():\n        encode_more=key\n        encode_less_list=value\n\n        more_toxic=id2text_dict[encode_more]\n        for encode_less in encode_less_list:\n            less_toxic=id2text_dict[encode_less]\n            row=pd.DataFrame({\"worker\":[999],\"less_toxic\":[less_toxic],\"more_toxic\":[more_toxic],\"encode_less\":[encode_less],\n                                       \"encode_more\":[encode_more]})\n            aug_df=aug_df.append(row,ignore_index=True)\n    work_list=np.array([999]*len(aug_df),dtype=np.int64)\n    aug_df[\"worker\"]=work_list\n    return aug_df","metadata":{"papermill":{"duration":0.056054,"end_time":"2021-12-23T03:28:54.410318","exception":false,"start_time":"2021-12-23T03:28:54.354264","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-12T13:37:28.970726Z","iopub.execute_input":"2022-01-12T13:37:28.97115Z","iopub.status.idle":"2022-01-12T13:37:28.997716Z","shell.execute_reply.started":"2022-01-12T13:37:28.971104Z","shell.execute_reply":"2022-01-12T13:37:28.996495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df,comments=generate_comments(data_df)\nif translate_aug==True:\n    comment_translation=pd.read_csv(\"../input/translate-toxic/comment_translation.csv\")\n    comment_translation=comment_translation.merge(comments,on=\"text\",how=\"left\")\nif data_aug==True:\n    aug_df=data_aug1(data_df,comments)\n    data_df=pd.concat([data_df,aug_df],axis=0)\n    data_df=data_df.reset_index(drop=True)","metadata":{"papermill":{"duration":0.755321,"end_time":"2021-12-23T03:28:55.200325","exception":false,"start_time":"2021-12-23T03:28:54.445004","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-12T13:37:29.003194Z","iopub.execute_input":"2022-01-12T13:37:29.003627Z","iopub.status.idle":"2022-01-12T13:37:29.150403Z","shell.execute_reply.started":"2022-01-12T13:37:29.003593Z","shell.execute_reply":"2022-01-12T13:37:29.149367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DATASET_TEST==True:\n    data_df=data_df[0:400]","metadata":{"papermill":{"duration":0.04379,"end_time":"2021-12-23T03:28:55.280908","exception":false,"start_time":"2021-12-23T03:28:55.237118","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-12T13:37:29.152323Z","iopub.execute_input":"2022-01-12T13:37:29.15269Z","iopub.status.idle":"2022-01-12T13:37:29.16021Z","shell.execute_reply.started":"2022-01-12T13:37:29.152649Z","shell.execute_reply":"2022-01-12T13:37:29.159055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"交叉","metadata":{"papermill":{"duration":0.033677,"end_time":"2021-12-23T03:28:55.348276","exception":false,"start_time":"2021-12-23T03:28:55.314599","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection import GroupKFold\nclass UnionFind():\n    def __init__(self, n):\n        self.n = n\n        self.parents = [-1] * n\n\n    def find(self, x):\n        if self.parents[x] < 0:\n            return x\n        else:\n            self.parents[x] = self.find(self.parents[x])\n            return self.parents[x]\n\n    def union(self, x, y):\n        x = self.find(x)\n        y = self.find(y)\n        if x == y:\n            return\n        if self.parents[x] > self.parents[y]:\n            x, y = y, x\n        self.parents[x] += self.parents[y]\n        self.parents[y] = x\n\n\ndef get_group_unionfind(train: pd.DataFrame):\n    less_unique_text = train['less_toxic'].unique()\n    more_unique_text = train['more_toxic'].unique()\n    unique_text = np.hstack([less_unique_text, more_unique_text])\n    unique_text = np.unique(unique_text).tolist()    \n    text2num = {text: i for i, text in enumerate(unique_text)}\n    num2text = {num: text for text, num in text2num.items()}\n    train['num_less_toxic'] = train['less_toxic'].map(text2num)\n    train['num_more_toxic'] = train['more_toxic'].map(text2num)\n\n    uf = UnionFind(len(unique_text))\n    for seq1, seq2 in train[['num_less_toxic', 'num_more_toxic']].to_numpy():\n        uf.union(seq1, seq2)\n\n    text2group = {num2text[i]: uf.find(i) for i in range(len(unique_text))}\n    train['group'] = train['less_toxic'].map(text2group)\n    train = train.drop(columns=['num_less_toxic', 'num_more_toxic'])\n    return train","metadata":{"execution":{"iopub.status.busy":"2022-01-12T13:37:29.161786Z","iopub.execute_input":"2022-01-12T13:37:29.162355Z","iopub.status.idle":"2022-01-12T13:37:29.179285Z","shell.execute_reply.started":"2022-01-12T13:37:29.162291Z","shell.execute_reply":"2022-01-12T13:37:29.178135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df = get_group_unionfind(data_df)\ngroup_kfold = GroupKFold(n_splits=CONFIG[\"fold_num\"])\nfor fold, (trn_idx, val_idx) in enumerate(group_kfold.split(data_df, data_df, data_df['group'])): \n    data_df.loc[val_idx , \"kfold\"] = fold\n\ndata_df[\"kfold\"] = data_df[\"kfold\"].astype(int)\ndata_df.to_csv('train_noleak.csv', index=False)\ndata_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-12T13:37:29.18121Z","iopub.execute_input":"2022-01-12T13:37:29.181675Z","iopub.status.idle":"2022-01-12T13:37:29.25026Z","shell.execute_reply.started":"2022-01-12T13:37:29.181628Z","shell.execute_reply":"2022-01-12T13:37:29.249075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# skf=StratifiedKFold(n_splits=CONFIG[\"fold_num\"],shuffle=True,random_state=CONFIG[\"seed\"])\n# for fold,(_,val_) in enumerate(skf.split(X=data_df,y=data_df.worker)):\n#     data_df.loc[val_,\"kfold\"]=int(fold)\n    \n# data_df[\"kfold\"]=data_df[\"kfold\"].astype(int)\n# data_df.head()","metadata":{"papermill":{"duration":0.104501,"end_time":"2021-12-23T03:28:55.486463","exception":false,"start_time":"2021-12-23T03:28:55.381962","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-12T13:37:29.252439Z","iopub.execute_input":"2022-01-12T13:37:29.252812Z","iopub.status.idle":"2022-01-12T13:37:29.258124Z","shell.execute_reply.started":"2022-01-12T13:37:29.252745Z","shell.execute_reply":"2022-01-12T13:37:29.257075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DatasetRetriever(Dataset):\n    def __init__(self,data,tokenizer,max_len=CONFIG[\"MAX_LENGTH\"]):\n        self.data=data\n        self.tokenizer=tokenizer\n        self.max_len=max_len\n        self.more_toxic=data[\"more_toxic\"].values\n        self.less_toxic=data[\"less_toxic\"].values\n        \n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, item):\n        more_toxic=self.more_toxic[item]\n        less_toxic=self.less_toxic[item]\n\n        features1=self.convert_examples_to_features(more_toxic)\n        features2=self.convert_examples_to_features(less_toxic)\n        features1={\"input_ids\":features1[\"input_ids\"],\"attention_mask\":features1[\"attention_mask\"]}\n        features2={\"input_ids\":features2[\"input_ids\"],\"attention_mask\":features2[\"attention_mask\"]}\n        target=1\n        return {\"more_toxic\":{key:torch.tensor(value,dtype=torch.long) for key,value in features1.items()},\n                \"less_toxic\":{key:torch.tensor(value,dtype=torch.long) for key,value in features2.items()},\n                \"target\":torch.tensor(target,dtype=torch.long)}\n    def convert_examples_to_features(self, example):\n        encoded = self.tokenizer.encode_plus(\n            example,\n            add_special_tokens=True,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_len,\n            is_split_into_words=False,\n            )\n        return encoded\ndef make_dataloader(data,batch_size,model_dir=MODEL_DIR,max_len=CONFIG[\"MAX_LENGTH\"]):\n    tokenizer=AutoTokenizer.from_pretrained(model_dir)\n    dataset=DatasetRetriever(data,tokenizer,max_len)\n    sampler=RandomSampler(dataset)\n    \n    dataloader=DataLoader(dataset,\n                          batch_size=batch_size,\n                          sampler=sampler\n                         )\n    return dataloader","metadata":{"papermill":{"duration":0.050813,"end_time":"2021-12-23T03:28:55.572476","exception":false,"start_time":"2021-12-23T03:28:55.521663","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-12T13:37:29.260174Z","iopub.execute_input":"2022-01-12T13:37:29.260865Z","iopub.status.idle":"2022-01-12T13:37:29.278275Z","shell.execute_reply.started":"2022-01-12T13:37:29.260818Z","shell.execute_reply":"2022-01-12T13:37:29.277007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_loaders(df,fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    if translate_aug==True:\n        df_train_encode=df_train.drop([\"less_toxic\",\"more_toxic\"],axis=1)\n        for language_text in translate_text:\n            temp_train=df_train_encode\n            \n            temp_train=temp_train.merge(comment_translation[[\"encode_text\",language_text]],left_on=\"encode_less\",right_on=\"encode_text\",how=\"left\")\n            temp_train=temp_train.rename(columns={language_text:\"less_toxic\"})\n            temp_train.drop([\"encode_text\"],axis=1,inplace=True)\n            \n            temp_train=temp_train.merge(comment_translation[[\"encode_text\",language_text]],left_on=\"encode_more\",right_on=\"encode_text\",how=\"left\")\n            temp_train=temp_train.rename(columns={language_text:\"more_toxic\"})\n            temp_train.drop([\"encode_text\"],axis=1,inplace=True)\n            df_train=pd.concat([df_train,temp_train])\n    train_loader=make_dataloader(df_train,CONFIG[\"TRAIN_BATCH_SIZE\"],MODEL_DIR,CONFIG[\"MAX_LENGTH\"])\n    \n    valid_loader=make_dataloader(df_valid,CONFIG[\"DEV_BATCH_SIZE\"],MODEL_DIR,CONFIG[\"MAX_LENGTH\"])\n    \n    return train_loader, valid_loader","metadata":{"papermill":{"duration":0.049317,"end_time":"2021-12-23T03:28:55.656104","exception":false,"start_time":"2021-12-23T03:28:55.606787","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-12T13:37:29.280166Z","iopub.execute_input":"2022-01-12T13:37:29.280595Z","iopub.status.idle":"2022-01-12T13:37:29.294922Z","shell.execute_reply.started":"2022-01-12T13:37:29.280498Z","shell.execute_reply":"2022-01-12T13:37:29.293943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"** 模型输出结构","metadata":{}},{"cell_type":"code","source":"class OriginModel(nn.Module):\n    def __init__(self,model_name):\n        super(OriginModel,self).__init__()\n        self.config=AutoConfig.from_pretrained(model_name)\n        self.config.update({\"hidden_dropout_prob\": 0.0,\"attention_probs_dropout_prob\":0.0\n            })   \n        self.model=AutoModel.from_pretrained(model_name,config=self.config)\n        self.drop=nn.Dropout(p=0)\n        \n        self.linear=nn.Linear(self.config.to_dict()[hidden_size],CONFIG[\"num_class\"])\n           \n        self.dense = nn.Linear(self.config.to_dict()[hidden_size], self.config.to_dict()[hidden_size])\n        self.activation = nn.Tanh()\n    def forward(self,input_ids,attention_mask):\n        out=self.model(input_ids=input_ids,attention_mask=attention_mask,output_hidden_states=False)\n        last_hidden_state = out[0]\n        cls_embeddings = last_hidden_state[:,0]\n        pooled_output = self.dense(cls_embeddings)\n        pooled_output = self.activation(pooled_output)\n        \n        out=self.drop(pooled_output)\n        \n        outputs=self.linear(out)\n        \n        return outputs","metadata":{"papermill":{"duration":0.045237,"end_time":"2021-12-23T03:28:55.735323","exception":false,"start_time":"2021-12-23T03:28:55.690086","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-12T13:37:29.296994Z","iopub.execute_input":"2022-01-12T13:37:29.297336Z","iopub.status.idle":"2022-01-12T13:37:29.310286Z","shell.execute_reply.started":"2022-01-12T13:37:29.297283Z","shell.execute_reply":"2022-01-12T13:37:29.309039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"       \nLAYER_START = 4   # for WeightedLayerPoolingModel\n\nHIDDEN_DIM_FC = 128    # for AttentionPooling\n\nclass LastLayerCLSModel(nn.Module):\n    \n    def __init__(self, model_name):\n        super().__init__()\n        \n        self.config = AutoConfig.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n        self.linear = nn.Linear(self.config.to_dict()[hidden_size], CONFIG[\"num_class\"])\n        self.loss = nn.MSELoss()\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        last_hidden_state = outputs[0]\n        cls_embeddings = last_hidden_state[:,0]\n        logits = self.linear(cls_embeddings)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        if labels is not None:\n            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n            return loss\n        else:\n            return preds\n        \n        \nclass MeanPoolingModel(nn.Module):\n    \n    def __init__(self, model_name):\n        super().__init__()\n        \n        self.config = AutoConfig.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n        self.linear = nn.Linear(self.config.to_dict()[hidden_size], 1)\n        self.loss = nn.MSELoss()\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        logits = self.linear(mean_embeddings)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        if labels is not None:\n            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n            return loss\n        else:\n            return preds\n        \n        \nclass MaxPoolingModel(nn.Module):\n    \n    def __init__(self, model_name):\n        super().__init__()\n        \n        self.config = AutoConfig.from_pretrained(model_name)\n        self.config.update({\"hidden_dropout_prob\": 0.0,\"attention_probs_dropout_prob\":0.0\n            })      \n        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n        self.linear = nn.Linear(self.config.to_dict()[hidden_size], 1)\n        self.loss = nn.MSELoss()\n        \n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        last_hidden_state[input_mask_expanded == 0] = -1e9   # large negative value\n        max_embeddings, _ = torch.max(last_hidden_state, 1)\n        logits = self.linear(max_embeddings)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        if labels is not None:\n            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n            return loss\n        else:\n            return preds\n        \n        \nclass SecondToLastLayerCLSModel(nn.Module):\n    \n    def __init__(self, model_name):\n        super().__init__()\n        \n        self.config = AutoConfig.from_pretrained(model_name)\n        self.config.update({'output_hidden_states':True})\n        self.config.update({\"hidden_dropout_prob\": 0.0,\"attention_probs_dropout_prob\":0.0\n            })      \n        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n        self.linear = nn.Linear(self.config.to_dict()[hidden_size], 1)\n        self.loss = nn.MSELoss()\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        all_hidden_states = torch.stack(outputs[2])\n        second_to_last_layer = self.config.to_dict()[num_hidden_layers]-2\n        cls_embeddings = all_hidden_states[second_to_last_layer,:,0]\n        logits = self.linear(cls_embeddings)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        if labels is not None:\n            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n            return loss\n        else:\n            return preds\n        \nclass ConcatenateLastFourModel(nn.Module):\n    \n    def __init__(self, model_name):\n        super().__init__()\n        \n        self.config = AutoConfig.from_pretrained(model_name)\n        self.config.update({'output_hidden_states':True})\n        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n        self.linear = nn.Linear(4*self.config.to_dict()[hidden_size], 1)\n        self.loss = nn.MSELoss()\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        all_hidden_states = torch.stack(outputs[2])\n        concatenate_pooling = torch.cat(\n            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]), -1\n        )\n        concatenate_pooling = concatenate_pooling[:,0]\n        logits = self.linear(concatenate_pooling)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        if labels is not None:\n            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n            return loss\n        else:\n            return preds\n        \n        \nclass WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_hidden_states):\n        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n        return weighted_average\n    \nclass WeightedLayerPoolingModel(nn.Module):\n    def __init__(self, model_name):\n        super().__init__()\n        \n        self.config = AutoConfig.from_pretrained(model_name)\n        self.config.update({'output_hidden_states':True})\n        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n        self.pooling = WeightedLayerPooling(self.config.to_dict()[num_hidden_layers], \n                                      layer_start=LAYER_START,\n                                      layer_weights=None)\n        self.layer_norm = nn.LayerNorm(self.config.to_dict()[hidden_size])\n        self.linear = nn.Linear(self.config.to_dict()[hidden_size], 1)\n        self.loss = nn.MSELoss()\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        all_hidden_states = torch.stack(outputs[2])\n        \n        weighted_pooling_embeddings = self.pooling(all_hidden_states)\n        weighted_pooling_embeddings = weighted_pooling_embeddings[:,0]\n        \n        norm_embeddings = self.layer_norm(weighted_pooling_embeddings)\n        logits = self.linear(norm_embeddings)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        if labels is not None:\n            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n            return loss\n        else:\n            return preds\n        \nclass AttentionPooling(nn.Module):\n    def __init__(self, num_layers, hidden_size, hiddendim_fc):\n        super(AttentionPooling, self).__init__()\n        self.num_hidden_layers = num_layers\n        self.hidden_size = hidden_size\n        self.hiddendim_fc = hiddendim_fc\n        self.dropout = nn.Dropout(0.1)\n\n        q_t = np.random.normal(loc=0.0, scale=0.1, size=(1, self.hidden_size))\n        self.q = nn.Parameter(torch.from_numpy(q_t)).float().to(DEVICE)\n        w_ht = np.random.normal(loc=0.0, scale=0.1, size=(self.hidden_size, self.hiddendim_fc))\n        self.w_h = nn.Parameter(torch.from_numpy(w_ht)).float().to(DEVICE)\n\n    def forward(self, all_hidden_states):\n        hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\n                                     for layer_i in range(1, self.num_hidden_layers+1)], dim=-1)\n        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n        out = self.attention(hidden_states)\n        out = self.dropout(out)\n        return out\n\n    def attention(self, h):\n        v = torch.matmul(self.q, h.transpose(-2, -1)).squeeze(1)\n        v = F.softmax(v, -1)\n        v_temp = torch.matmul(v.unsqueeze(1), h).transpose(-2, -1)\n        v = torch.matmul(self.w_h.transpose(1, 0), v_temp).squeeze(2)\n        return v\n\nclass AttentionPoolingModel(nn.Module):\n    def __init__(self, model_name):\n        super().__init__()\n        \n        self.config = AutoConfig.from_pretrained(model_name)\n        self.config.update({'output_hidden_states':True})\n        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n        self.pooler = AttentionPooling(self.config.to_dict()[num_hidden_layers], self.config.to_dict()[hidden_size], HIDDEN_DIM_FC)\n        \n        self.linear = nn.Linear(HIDDEN_DIM_FC, 1)\n        self.loss = nn.MSELoss()\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        all_hidden_states = torch.stack(outputs[2])\n        \n        attention_pooling_embeddings = self.pooler(all_hidden_states)\n        \n        logits = self.linear(attention_pooling_embeddings)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        if labels is not None:\n            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n            return loss\n        else:\n            return preds\n        \n# class LitModel(nn.Module):\n#     def __init__(self, n_hidden_state=4):\n#         super().__init__()\n\n#         self.config = AutoConfig.from_pretrained(model_name)\n#         config.update({\"output_hidden_states\":True, \n#                        \"hidden_dropout_prob\": 0.0,\n#                        \"layer_norm_eps\": 1e-7})                       \n        \n#         self.roberta = AutoModel.from_pretrained(model_name, config=self.config)\n#         self.n_hidden_state = self.config.to_dict()[hidden_size]\n        \n#         for i in range(self.n_hidden_state):\n#             layer_name = f\"attention_{i+1}\"\n#             layer = nn.Sequential(\n#                 nn.Linear(768, 512),\n#                 nn.Tanh(),                       \n#                 nn.Linear(512, 1),\n#                 nn.Softmax(dim=1)\n#             )\n#             setattr(self, layer_name, layer)\n\n#         self.regressor = nn.Sequential(                        \n#             nn.Linear(768, 1)                        \n#         )\n        \n\n#     def forward(self, input_ids, attention_mask):\n#         roberta_output = self.roberta(input_ids=input_ids,\n#                                       attention_mask=attention_mask)\n        \n#         weights = [\n#             getattr(self, f\"attention_{i+1}\")(roberta_output.hidden_states[-(i+1)]) for i in range(self.n_hidden_state)\n#         ]\n        \n#         context_vectors = [\n#             torch.sum(weights[i] * roberta_output.hidden_states[-(i+1)], dim=1) for i in range(self.n_hidden_state)\n#         ]\n        \n#         stacked = torch.stack(context_vectors)\n#         hidden = torch.sum(stacked, dim=0) / self.n_hidden_state\n        \n#         # Now we reduce the context vector to the prediction score.\n#         return self.regressor(hidden)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T13:37:29.312515Z","iopub.execute_input":"2022-01-12T13:37:29.313045Z","iopub.status.idle":"2022-01-12T13:37:29.381583Z","shell.execute_reply.started":"2022-01-12T13:37:29.312997Z","shell.execute_reply":"2022-01-12T13:37:29.380296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"func_dict={\"OriginModel\":OriginModel,\"LastLayerCLSModel\":LastLayerCLSModel,\"MeanPoolingModel\":MeanPoolingModel,\n           \"MaxPoolingModel\":MaxPoolingModel,\"SecondToLastLayerCLSModel\":SecondToLastLayerCLSModel,\"ConcatenateLastFourModel\":\n           ConcatenateLastFourModel,\"WeightedLayerPoolingModel\":WeightedLayerPoolingModel,\"AttentionPoolingModel\":AttentionPoolingModel}\nJigsawModel=func_dict.get(model_struct)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-12T13:37:29.383554Z","iopub.execute_input":"2022-01-12T13:37:29.383999Z","iopub.status.idle":"2022-01-12T13:37:29.396664Z","shell.execute_reply.started":"2022-01-12T13:37:29.383955Z","shell.execute_reply":"2022-01-12T13:37:29.395532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def criterion(outputs1, outputs2, targets):\n    return nn.MarginRankingLoss(margin=CONFIG[\"margin\"])(outputs1, outputs2, targets)","metadata":{"papermill":{"duration":0.041048,"end_time":"2021-12-23T03:28:55.810694","exception":false,"start_time":"2021-12-23T03:28:55.769646","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-12T13:37:29.399069Z","iopub.execute_input":"2022-01-12T13:37:29.399314Z","iopub.status.idle":"2022-01-12T13:37:29.412668Z","shell.execute_reply.started":"2022-01-12T13:37:29.399285Z","shell.execute_reply":"2022-01-12T13:37:29.411563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_parameters(model,model_init_lr,multiplier, classifier_lr):\n    #权重分层，越靠近下游学习率越高\n    parameters=[]\n    lr=model_init_lr\n    # 迭代器包含 层名字和参数 parameters()函数只包含参数\n    #定义的层字典，参数的key必须叫params，否则在optimizer 父类中冲突\n    for layer in range(model.config.to_dict()[num_hidden_layers]-1,-1,-1):\n        layer_parameters={\n            \"params\":[p for n,p in model.named_parameters() if f\"encoder.layer.{layer}.\" in n],\n            \"lr\":lr\n        }\n        lr*=multiplier\n        parameters.append(layer_parameters)\n    \n    \n    classify_parameters={\n        #自己定义了什么分类层在此更改名字\n        \"params\":[p for n,p in model.named_parameters() if \"linear\"  in n],\n        \"lr\":classifier_lr\n    }\n        \n    parameters.append(classify_parameters)\n    return parameters","metadata":{"papermill":{"duration":0.045658,"end_time":"2021-12-23T03:28:55.889878","exception":false,"start_time":"2021-12-23T03:28:55.84422","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-12T13:57:07.560862Z","iopub.execute_input":"2022-01-12T13:57:07.561451Z","iopub.status.idle":"2022-01-12T13:57:07.569247Z","shell.execute_reply.started":"2022-01-12T13:57:07.561415Z","shell.execute_reply":"2022-01-12T13:57:07.568248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_scheduler(optimizer):\n    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n                                                   eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n                                                             eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == None:\n        return None\n        \n    return scheduler","metadata":{"papermill":{"duration":0.042839,"end_time":"2021-12-23T03:28:55.96709","exception":false,"start_time":"2021-12-23T03:28:55.924251","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-12T13:57:08.372361Z","iopub.execute_input":"2022-01-12T13:57:08.372891Z","iopub.status.idle":"2022-01-12T13:57:08.379177Z","shell.execute_reply.started":"2022-01-12T13:57:08.372855Z","shell.execute_reply":"2022-01-12T13:57:08.378149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model,dev_dataloader):\n    model.eval()\n    dev_loss=0\n    for index,batch in enumerate(dev_dataloader):\n        \n        more_toxic_inputs=batch[\"more_toxic\"]\n        less_toxic_inputs=batch[\"less_toxic\"]\n        target=batch[\"target\"].to(DEVICE)\n\n        more_toxic_inputs={key: value.to(DEVICE) for key,value in more_toxic_inputs.items()}\n        less_toxic_inputs={key: value.to(DEVICE) for key,value in less_toxic_inputs.items()}\n        with torch.no_grad():\n            out_more=model(**more_toxic_inputs)\n            out_less=model(**less_toxic_inputs)\n\n            loss=criterion(out_more, out_less, target)\n        \n            dev_loss+=loss.item()\n        \n    return dev_loss/len(dev_dataloader)\ndef train(model,train_dataloader,dev_dataloader,evaluate_step=None,swa_start=None):\n\n    if run_db==True:\n        wandb.watch(model,log_freq=100)\n#     optimizer=AdamW(get_parameters(model, model_init_lr=CONFIG[\"model_init_lr\"], multiplier=CONFIG[\"multiplier\"], \n#                                    classifier_lr=CONFIG[\"classifier_lr\"]),\n#                     lr = CONFIG['LR'], eps = CONFIG['EPS'],weight_decay=CONFIG['weight_decay'])\n    optimizer = AdamW(model.parameters(),lr= CONFIG['LR'], eps = CONFIG['EPS'],weight_decay=CONFIG['weight_decay'])\n    if evaluate_step==None:\n        evaluate_step=len(train_dataloader)\n    if swa_use==True:\n        swa_model=AveragedModel(model).to(DEVICE)\n        swa_scheduler = SWALR(optimizer, swa_lr=CONFIG[\"swa_lr\"])\n    \"\"\"\n    get_linear_schedule_with_warmup:学习率先从0开始warm_up到设定学习率，再逐渐减到0\n    num_warmup_steps：完成预热的步数\n    num_training_steps：训练批次*epochs 训练的step数\n    \"\"\"\n    scheduler = fetch_scheduler(optimizer)\n    if scheduler==None:\n        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, \n                                                num_training_steps=len(train_dataloader) * CONFIG[\"EPOCHS\"])\n    best_val_loss=100\n    best_model_param=None\n\n    scaler = GradScaler()\n    start=time.time()\n    for epoch in range(CONFIG[\"EPOCHS\"]):\n        print(f\"\\n Epoch{epoch} train start \\n\")\n        train_loss=0\n        model.train()\n        #total 更新进度 \n        bar=tqdm(enumerate(train_dataloader),total=len(train_dataloader))\n        for index,batch in bar:\n            model.zero_grad()\n            more_toxic_inputs=batch[\"more_toxic\"]\n            less_toxic_inputs=batch[\"less_toxic\"]\n            target=batch[\"target\"].to(DEVICE)\n\n            more_toxic_inputs={key: value.to(DEVICE) for key,value in more_toxic_inputs.items()}\n            less_toxic_inputs={key: value.to(DEVICE) for key,value in less_toxic_inputs.items()}\n            if FP16==True:\n                with autocast():\n                    out_more=model(**more_toxic_inputs)\n                    out_less=model(**less_toxic_inputs)\n                    loss=criterion(out_more, out_less, target)\n\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                out_more=model(**more_toxic_inputs)\n                out_less=model(**less_toxic_inputs)\n                loss=criterion(out_more, out_less, target)\n                loss.backward()\n                optimizer.step()\n            if swa_use==True and epoch>=swa_start-1:\n\n                swa_model.update_parameters(model)\n                swa_scheduler.step()\n            else:\n                scheduler.step()\n\n            train_loss+=loss.item()\n            if (index+1)%evaluate_step==0 or (index+1)==len(train_dataloader):\n                if swa_use==True and epoch>=swa_start-1:\n                    val_loss=evaluate(swa_model,dev_dataloader)\n                else:\n                    val_loss=evaluate(model,dev_dataloader)\n\n                if run_db==True:\n                    wandb.log({\"Train LOSS\":loss})\n                    wandb.log({\"Valid LOSS\":val_loss})\n\n                if val_loss<best_val_loss:\n                    best_val_loss=val_loss\n                    if swa_use==True and epoch>=swa_start-1:\n                        best_model_param=swa_model.module.state_dict()\n                    else:\n                        best_model_param=model.state_dict()\n                    print(f\"best_model saved ,val_loss:{best_val_loss}\")\n        avg_train_loss=train_loss/len(train_dataloader)\n        print(f\"EPOCH:{epoch+1},train_loss:{avg_train_loss},val_loss:{val_loss}\")\n\n    end=time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n    time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    if run_db==True:\n        run.summary[\"time (hour)\"]=time_elapsed /3600\n    return best_val_loss,best_model_param","metadata":{"papermill":{"duration":0.062661,"end_time":"2021-12-23T03:28:56.063675","exception":false,"start_time":"2021-12-23T03:28:56.001014","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-12T13:57:10.685937Z","iopub.execute_input":"2022-01-12T13:57:10.686242Z","iopub.status.idle":"2022-01-12T13:57:10.715032Z","shell.execute_reply.started":"2022-01-12T13:57:10.68621Z","shell.execute_reply":"2022-01-12T13:57:10.71399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold in range(CONFIG[\"fold_num\"]):\n    print(f\"\\n Fold{fold} train start\")\n    if run_db==True:\n        run = wandb.init(project='Jigsaw', \n                     config=CONFIG,\n                     job_type='Train',\n                     group=CONFIG['group'],\n                     tags=['roberta-base', f'{HASH_NAME}', 'margin-loss'],\n                     name=f'{HASH_NAME}-fold-{fold}',\n                     anonymous='must')\n    train_loader,dev_loader=prepare_loaders(data_df,fold)    \n    model=JigsawModel(MODEL_DIR)\n    model.to(DEVICE)\n    dev_loss,best_model_param=train(model,train_loader,dev_loader,evaluate_step=CONFIG[\"evaluate_step\"],swa_start=CONFIG[\"swa_start\"])\n    model_path=f\"bestmodel-{fold}.pth\"\n    torch.save(best_model_param,model_path)\n    if run_db==True:\n        run.finish()\n    \n    del model,train_loader,dev_loader        \n    gc.collect()","metadata":{"papermill":{"duration":24312.182609,"end_time":"2021-12-23T10:14:08.280605","exception":false,"start_time":"2021-12-23T03:28:56.097996","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-12T13:57:11.036772Z","iopub.execute_input":"2022-01-12T13:57:11.037368Z","iopub.status.idle":"2022-01-12T13:58:12.072891Z","shell.execute_reply.started":"2022-01-12T13:57:11.037333Z","shell.execute_reply":"2022-01-12T13:58:12.071616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# cv","metadata":{"papermill":{"duration":0.046284,"end_time":"2021-12-23T10:14:08.373253","exception":false,"start_time":"2021-12-23T10:14:08.326969","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class DatasetRetriever_cv(Dataset):\n    def __init__(self,data,tokenizer,max_len=CONFIG[\"MAX_LENGTH\"]):\n        self.data=data\n        self.tokenizer=tokenizer\n        self.max_len=max_len\n        self.text=self.data[\"text\"].values\n        \n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, item):\n        text=self.text[item]\n\n        features1=self.convert_examples_to_features(text)\n        features1={\"input_ids\":features1[\"input_ids\"],\"attention_mask\":features1[\"attention_mask\"]}\n\n        return {\"text\":{key:torch.tensor(value,dtype=torch.long) for key,value in features1.items()}}\n    def convert_examples_to_features(self, example):\n        encoded = self.tokenizer.encode_plus(\n            example,\n            add_special_tokens=True,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_len,\n            is_split_into_words=False,\n            )\n        return encoded\ndef make_dataloader_cv(data,batch_size,model_dir=MODEL_DIR,max_len=CONFIG[\"MAX_LENGTH\"]):\n    tokenizer=AutoTokenizer.from_pretrained(model_dir)\n    dataset=DatasetRetriever_cv(data,tokenizer,max_len)\n    sampler=SequentialSampler(dataset)\n    \n    dataloader=DataLoader(dataset,\n                          batch_size=batch_size,\n                          sampler=sampler\n                         )\n    return dataloader\ndef evaluate_cv(model,test_dataloader):\n    model.eval()\n    Preds=[]\n    for index,batch in enumerate(test_dataloader):\n        \n        text_inputs=batch[\"text\"]\n        \n        text_inputs={key: value.to(DEVICE) for key,value in text_inputs.items()}\n        with torch.no_grad():\n            out_more=model(**text_inputs)\n            Preds.append(out_more.view(-1).cpu().detach().numpy())\n    \n    Preds = np.concatenate(Preds) \n    gc.collect()\n    \n    return Preds\n\ndef inference(model_paths,data_df,comments):\n    \n    for fold in range(CONFIG[\"fold_num\"]):\n        print(f\"fold{fold} dev start\")\n\n        data_fold=data_df[data_df.kfold == fold]\n#         data_fold.drop([\"label_min\",\"label_max\",\"win_min\",\"win_max\"],axis=1,inplace=True)\n    \n        comments_fold_id=np.concatenate((data_fold[\"encode_more\"].values,data_fold[\"encode_less\"].values))\n        comments_fold_id=np.unique(comments_fold_id)\n        select_fold_list=comments.apply(lambda row : True if row[\"encode_text\"] in comments_fold_id else False ,axis=1)\n        comments_fold=comments[select_fold_list]\n        comments_fold[\"access_time\"]=comments_fold[\"access_time\"]+1\n        \n        \n        test_loader=make_dataloader_cv(comments_fold,CONFIG[\"DEV_BATCH_SIZE\"],MODEL_DIR,CONFIG[\"MAX_LENGTH\"])\n        model=JigsawModel(MODEL_DIR)\n        model.to(DEVICE)\n        path=model_paths[fold]\n        \n        model.load_state_dict(torch.load(path))\n        preds = evaluate_cv(model, test_loader)\n        comments_fold[\"toxic_value\"]=comments_fold[\"toxic_value\"]+preds\n        \n        data_df.loc[data_fold.index]=data_fold\n        comments.loc[comments_fold.index]=comments_fold\n\n        del model,test_loader        \n    \n    return data_df,comments","metadata":{"papermill":{"duration":0.070172,"end_time":"2021-12-23T10:14:08.490594","exception":false,"start_time":"2021-12-23T10:14:08.420422","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if run_db==True:\n    run = wandb.init(project='Jigsaw', \n             config=CONFIG,\n             job_type='cv',\n             group=CONFIG['group'],\n             tags=['roberta-base', f'{HASH_NAME}', 'margin-loss'],\n             name=f'{HASH_NAME}-fold-{fold}',\n             anonymous='must')\nMODEL_PATHS=[f\"bestmodel-{num}.pth\" for num in range(CONFIG[\"fold_num\"])]\n# MODEL_PATHS=[f\"../input/baseline1-toxic-value/bestmodel-{num}.pth\" for num in range(CONFIG[\"fold_num\"])]\ndata_df,comments= inference(MODEL_PATHS, data_df,comments)\n\ncomments[\"toxic_value\"]=comments[\"toxic_value\"]/comments[\"access_time\"]\ncomments.index=comments[\"encode_text\"]\nindex_score_dict=comments.to_dict()[\"toxic_value\"]\ndata_df[\"less_value\"]=data_df[\"encode_less\"].map(lambda x:index_score_dict[x])\ndata_df[\"more_value\"]=data_df[\"encode_more\"].map(lambda x:index_score_dict[x])\ndata_df[\"pair_True\"]=data_df.apply(lambda row:True if row[\"more_value\"]>row[\"less_value\"] else False,axis=1)\ncv=data_df[\"pair_True\"].mean()\nprint(cv)\nif run_db==True:\n    wandb.log({\"cv\":data_df[\"pair_True\"].mean()})\n    run.finish()","metadata":{"papermill":{"duration":194.831052,"end_time":"2021-12-23T10:17:23.369397","exception":false,"start_time":"2021-12-23T10:14:08.538345","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# jc_df=pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\n\n# min_len = (jc_df['toxic'] == 1).sum()\n# df_y0_undersample = jc_df[jc_df['toxic'] == 0].sample(n=min_len, random_state=201)\n# comments_fold = pd.concat([jc_df[jc_df['toxic'] == 1], df_y0_undersample])\n\n# comments_fold.rename(columns={\"comment_text\":\"text\"},inplace=True)\n# comments_fold[\"toxic_value\"]=0","metadata":{"papermill":{"duration":2.195751,"end_time":"2021-12-23T10:17:25.614408","exception":false,"start_time":"2021-12-23T10:17:23.418657","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for fold in range(CONFIG[\"fold_num\"]):\n#     print(f\"fold{fold} dev start\")\n#     test_loader=make_dataloader_cv(comments_fold,CONFIG[\"DEV_BATCH_SIZE\"],MODEL_DIR,CONFIG[\"MAX_LENGTH\"])\n#     model=JigsawModel(MODEL_DIR)\n#     model.to(DEVICE)\n#     path=MODEL_PATHS[fold]\n\n#     model.load_state_dict(torch.load(path))\n#     preds = evaluate_cv(model, test_loader)\n#     comments_fold[\"toxic_value\"]=comments_fold[\"toxic_value\"]+preds\n#     del model,test_loader","metadata":{"papermill":{"duration":666.119257,"end_time":"2021-12-23T10:28:31.783306","exception":false,"start_time":"2021-12-23T10:17:25.664049","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# toxicSeperateValue=comments_fold[\"toxic_value\"].min()+(comments_fold[\"toxic_value\"].max()-comments_fold[\"toxic_value\"].min())/2\n# comments_fold[\"toxic_predict\"]=comments_fold.apply(lambda row : 1 if row[\"toxic_value\"]>=toxicSeperateValue else 0,axis=1)\n# comments_fold[\"predict_acc\"]=comments_fold.apply(lambda row : True if row[\"toxic_predict\"]==row[\"toxic\"] else False,axis=1)\n# cv=comments_fold[\"predict_acc\"].mean()\n# print(\"cv in first competition data:\",cv)\n# if run_db==True:\n#     wandb.log({\"cv in first competition \":cv})\n#     run.finish()","metadata":{"papermill":{"duration":1.114385,"end_time":"2021-12-23T10:28:32.946363","exception":false,"start_time":"2021-12-23T10:28:31.831978","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-03T05:02:13.332181Z","iopub.execute_input":"2022-01-03T05:02:13.332672Z","iopub.status.idle":"2022-01-03T05:02:13.339008Z","shell.execute_reply.started":"2022-01-03T05:02:13.332597Z","shell.execute_reply":"2022-01-03T05:02:13.33828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df.to_csv(f\"data_df_aug1\")","metadata":{"papermill":{"duration":1.124871,"end_time":"2021-12-23T10:28:34.120907","exception":false,"start_time":"2021-12-23T10:28:32.996036","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-03T05:02:13.340309Z","iopub.execute_input":"2022-01-03T05:02:13.340893Z","iopub.status.idle":"2022-01-03T05:02:13.36989Z","shell.execute_reply.started":"2022-01-03T05:02:13.340855Z","shell.execute_reply":"2022-01-03T05:02:13.369248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.049134,"end_time":"2021-12-23T10:28:34.218953","exception":false,"start_time":"2021-12-23T10:28:34.169819","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}